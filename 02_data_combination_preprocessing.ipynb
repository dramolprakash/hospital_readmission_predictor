{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc53b70",
   "metadata": {},
   "source": [
    "### Block 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dda0296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "Pandas version: 2.2.3\n",
      "NumPy version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dfe587",
   "metadata": {},
   "source": [
    "### Block 2: Load Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8a4684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed files from Phase A...\n",
      "Loading beneficiary files...\n",
      "Loading inpatient claims...\n",
      "✓ All files loaded successfully!\n",
      "├── Beneficiary 2008: 116,352 rows\n",
      "├── Beneficiary 2009: 114,538 rows\n",
      "├── Beneficiary 2010: 112,754 rows\n",
      "└── Inpatient Claims: 66,773 rows\n"
     ]
    }
   ],
   "source": [
    "# Load processed files from Notebook 1\n",
    "import os\n",
    "\n",
    "processed_path = \"data/processed/\"\n",
    "\n",
    "print(\"Loading processed files from Phase A...\")\n",
    "\n",
    "# Load beneficiary files\n",
    "print(\"Loading beneficiary files...\")\n",
    "df_ben_2008 = pd.read_parquet(f\"{processed_path}beneficiary_2008_processed.parquet\")\n",
    "df_ben_2009 = pd.read_parquet(f\"{processed_path}beneficiary_2009_processed.parquet\") \n",
    "df_ben_2010 = pd.read_parquet(f\"{processed_path}beneficiary_2010_processed.parquet\")\n",
    "\n",
    "# Load inpatient claims\n",
    "print(\"Loading inpatient claims...\")\n",
    "df_inpatient = pd.read_parquet(f\"{processed_path}inpatient_claims_processed.parquet\")\n",
    "\n",
    "print(\"✓ All files loaded successfully!\")\n",
    "print(f\"├── Beneficiary 2008: {df_ben_2008.shape[0]:,} rows\")\n",
    "print(f\"├── Beneficiary 2009: {df_ben_2009.shape[0]:,} rows\") \n",
    "print(f\"├── Beneficiary 2010: {df_ben_2010.shape[0]:,} rows\")\n",
    "print(f\"└── Inpatient Claims: {df_inpatient.shape[0]:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72697bb",
   "metadata": {},
   "source": [
    "### Block 3: Beneficiary Data Analysis Before Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501b0fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BENEFICIARY DATA ANALYSIS BEFORE COMBINATION\n",
      "==================================================\n",
      "BENEFICIARY OVERLAP ANALYSIS:\n",
      "2008 only: 1,814\n",
      "2009 only: 0\n",
      "2010 only: 0\n",
      "All 3 years: 112,754\n",
      "2008-2009 only: 1,784\n",
      "2009-2010 only: 0\n",
      "\n",
      "DEATH ANALYSIS:\n",
      "Beneficiaries who died in 2008: 1,814\n",
      "Beneficiaries who died in 2009: 1,784\n",
      "Death records in 2008 file: 1,814\n",
      "Death records in 2009 file: 1,784\n",
      "Death records in 2010 file: 1,863\n"
     ]
    }
   ],
   "source": [
    "print(\"BENEFICIARY DATA ANALYSIS BEFORE COMBINATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze beneficiary overlap across years\n",
    "ben_2008_ids = set(df_ben_2008['DESYNPUF_ID'])\n",
    "ben_2009_ids = set(df_ben_2009['DESYNPUF_ID'])\n",
    "ben_2010_ids = set(df_ben_2010['DESYNPUF_ID'])\n",
    "\n",
    "print(\"BENEFICIARY OVERLAP ANALYSIS:\")\n",
    "print(f\"2008 only: {len(ben_2008_ids - ben_2009_ids - ben_2010_ids):,}\")\n",
    "print(f\"2009 only: {len(ben_2009_ids - ben_2008_ids - ben_2010_ids):,}\")\n",
    "print(f\"2010 only: {len(ben_2010_ids - ben_2008_ids - ben_2009_ids):,}\")\n",
    "print(f\"All 3 years: {len(ben_2008_ids & ben_2009_ids & ben_2010_ids):,}\")\n",
    "print(f\"2008-2009 only: {len((ben_2008_ids & ben_2009_ids) - ben_2010_ids):,}\")\n",
    "print(f\"2009-2010 only: {len((ben_2009_ids & ben_2010_ids) - ben_2008_ids):,}\")\n",
    "\n",
    "# Check for deaths (beneficiaries who disappear)\n",
    "deaths_2008 = ben_2008_ids - ben_2009_ids\n",
    "deaths_2009 = ben_2009_ids - ben_2010_ids\n",
    "\n",
    "print(f\"\\nDEATH ANALYSIS:\")\n",
    "print(f\"Beneficiaries who died in 2008: {len(deaths_2008):,}\")\n",
    "print(f\"Beneficiaries who died in 2009: {len(deaths_2009):,}\")\n",
    "\n",
    "# Verify death records\n",
    "death_records_2008 = df_ben_2008[df_ben_2008['BENE_DEATH_DT'].notna()]\n",
    "death_records_2009 = df_ben_2009[df_ben_2009['BENE_DEATH_DT'].notna()]\n",
    "death_records_2010 = df_ben_2010[df_ben_2010['BENE_DEATH_DT'].notna()]\n",
    "\n",
    "print(f\"Death records in 2008 file: {len(death_records_2008):,}\")\n",
    "print(f\"Death records in 2009 file: {len(death_records_2009):,}\")\n",
    "print(f\"Death records in 2010 file: {len(death_records_2010):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005cecd",
   "metadata": {},
   "source": [
    "### Block 4: Create Master Beneficiary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998aee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING MASTER BENEFICIARY DATASET\n",
      "========================================\n",
      "Adding year indicators...\n",
      "├── 2008: 116352 records\n",
      "├── 2009: 114538 records\n",
      "└── 2010: 112754 records\n",
      "\n",
      "Combined dataset: 343,644 total records\n",
      "Unique beneficiaries: 116,352\n",
      "\n",
      "SAMPLE OF MASTER BENEFICIARY DATASET:\n",
      "             DESYNPUF_ID  BENE_YEAR  BENE_DEATH_DT  SP_CHF  SP_DIABETES\n",
      "0       00013D2EFD8E45D1       2008            NaN   False        False\n",
      "116352  00013D2EFD8E45D1       2009            NaN   False        False\n",
      "230890  00013D2EFD8E45D1       2010            NaN    True        False\n",
      "1       00016F745862898F       2008            NaN   False        False\n",
      "116353  00016F745862898F       2009            NaN   False         True\n",
      "230891  00016F745862898F       2010            NaN    True         True\n",
      "2       0001FDD721E223DC       2008            NaN   False        False\n",
      "116354  0001FDD721E223DC       2009            NaN   False        False\n",
      "230892  0001FDD721E223DC       2010            NaN   False        False\n",
      "3       00021CA6FF03E670       2008            NaN   False        False\n"
     ]
    }
   ],
   "source": [
    "def create_master_beneficiary_dataset():\n",
    "    \"\"\"\n",
    "    Combine beneficiary files from 2008-2010 into master dataset\n",
    "    Handle deaths and create longitudinal beneficiary records\n",
    "    \"\"\"\n",
    "    print(\"CREATING MASTER BENEFICIARY DATASET\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Add year indicators\n",
    "    df_2008 = df_ben_2008.copy()\n",
    "    df_2009 = df_ben_2009.copy() \n",
    "    df_2010 = df_ben_2010.copy()\n",
    "    \n",
    "    df_2008['BENE_YEAR'] = 2008\n",
    "    df_2009['BENE_YEAR'] = 2009\n",
    "    df_2010['BENE_YEAR'] = 2010\n",
    "    \n",
    "    print(f\"Adding year indicators...\")\n",
    "    print(f\"├── 2008: {len(df_2008)} records\")\n",
    "    print(f\"├── 2009: {len(df_2009)} records\")\n",
    "    print(f\"└── 2010: {len(df_2010)} records\")\n",
    "    \n",
    "    # Combine all years\n",
    "    df_master = pd.concat([df_2008, df_2009, df_2010], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nCombined dataset: {len(df_master):,} total records\")\n",
    "    print(f\"Unique beneficiaries: {df_master['DESYNPUF_ID'].nunique():,}\")\n",
    "    \n",
    "    # Sort by beneficiary and year\n",
    "    df_master = df_master.sort_values(['DESYNPUF_ID', 'BENE_YEAR'])\n",
    "    \n",
    "    return df_master\n",
    "\n",
    "# Create master dataset\n",
    "df_beneficiary_master = create_master_beneficiary_dataset()\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nSAMPLE OF MASTER BENEFICIARY DATASET:\")\n",
    "print(df_beneficiary_master[['DESYNPUF_ID', 'BENE_YEAR', 'BENE_DEATH_DT', 'SP_CHF', 'SP_DIABETES']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8421e9",
   "metadata": {},
   "source": [
    "### Block 5: Handle Missing Values Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e52bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MISSING VALUES ANALYSIS - MASTER BENEFICIARY\n",
      "==================================================\n",
      "Columns with missing values: 1\n",
      "               Missing_Count  Missing_Percentage Data_Type\n",
      "BENE_DEATH_DT         338183           98.410855   float64\n",
      "\n",
      "MISSING VALUE CATEGORIES:\n",
      "├── High missing (>50%): 1 columns\n",
      "├── Medium missing (5-50%): 0 columns\n",
      "└── Low missing (≤5%): 0 columns\n",
      "\n",
      "MISSING VALUES ANALYSIS - INPATIENT CLAIMS\n",
      "==================================================\n",
      "Columns with missing values: 70\n",
      "                        Missing_Count  Missing_Percentage Data_Type\n",
      "HCPCS_CD_29                     66773          100.000000   float64\n",
      "HCPCS_CD_30                     66773          100.000000   float64\n",
      "HCPCS_CD_31                     66773          100.000000   float64\n",
      "HCPCS_CD_32                     66773          100.000000   float64\n",
      "HCPCS_CD_2                      66773          100.000000   float64\n",
      "HCPCS_CD_1                      66773          100.000000   float64\n",
      "HCPCS_CD_5                      66773          100.000000   float64\n",
      "HCPCS_CD_6                      66773          100.000000   float64\n",
      "HCPCS_CD_4                      66773          100.000000   float64\n",
      "HCPCS_CD_3                      66773          100.000000   float64\n",
      "HCPCS_CD_11                     66773          100.000000   float64\n",
      "HCPCS_CD_12                     66773          100.000000   float64\n",
      "HCPCS_CD_9                      66773          100.000000   float64\n",
      "HCPCS_CD_10                     66773          100.000000   float64\n",
      "HCPCS_CD_13                     66773          100.000000   float64\n",
      "HCPCS_CD_14                     66773          100.000000   float64\n",
      "HCPCS_CD_8                      66773          100.000000   float64\n",
      "HCPCS_CD_7                      66773          100.000000   float64\n",
      "HCPCS_CD_39                     66773          100.000000   float64\n",
      "HCPCS_CD_40                     66773          100.000000   float64\n",
      "HCPCS_CD_33                     66773          100.000000   float64\n",
      "HCPCS_CD_34                     66773          100.000000   float64\n",
      "HCPCS_CD_35                     66773          100.000000   float64\n",
      "HCPCS_CD_36                     66773          100.000000   float64\n",
      "HCPCS_CD_37                     66773          100.000000   float64\n",
      "HCPCS_CD_38                     66773          100.000000   float64\n",
      "HCPCS_CD_43                     66773          100.000000   float64\n",
      "HCPCS_CD_44                     66773          100.000000   float64\n",
      "HCPCS_CD_41                     66773          100.000000   float64\n",
      "HCPCS_CD_42                     66773          100.000000   float64\n",
      "HCPCS_CD_45                     66773          100.000000   float64\n",
      "HCPCS_CD_17                     66773          100.000000   float64\n",
      "HCPCS_CD_16                     66773          100.000000   float64\n",
      "HCPCS_CD_15                     66773          100.000000   float64\n",
      "HCPCS_CD_22                     66773          100.000000   float64\n",
      "HCPCS_CD_21                     66773          100.000000   float64\n",
      "HCPCS_CD_28                     66773          100.000000   float64\n",
      "HCPCS_CD_27                     66773          100.000000   float64\n",
      "HCPCS_CD_26                     66773          100.000000   float64\n",
      "HCPCS_CD_25                     66773          100.000000   float64\n",
      "HCPCS_CD_24                     66773          100.000000   float64\n",
      "HCPCS_CD_23                     66773          100.000000   float64\n",
      "HCPCS_CD_18                     66773          100.000000   float64\n",
      "HCPCS_CD_19                     66773          100.000000   float64\n",
      "HCPCS_CD_20                     66773          100.000000   float64\n",
      "ICD9_PRCDR_CD_6                 62151           93.078041  category\n",
      "ICD9_DGNS_CD_10                 61318           91.830530  category\n",
      "ICD9_PRCDR_CD_5                 60327           90.346397  category\n",
      "OT_PHYSN_NPI                    59090           88.493852   float64\n",
      "ICD9_PRCDR_CD_4                 57396           85.956899  category\n",
      "ICD9_PRCDR_CD_3                 52311           78.341545  category\n",
      "ICD9_PRCDR_CD_2                 44040           65.954802  category\n",
      "ICD9_PRCDR_CD_1                 28542           42.744822  category\n",
      "OP_PHYSN_NPI                    27715           41.506297   float64\n",
      "ICD9_DGNS_CD_9                  21741           32.559567  category\n",
      "ICD9_DGNS_CD_8                  16826           25.198808  category\n",
      "ICD9_DGNS_CD_7                  12366           18.519461  category\n",
      "ICD9_DGNS_CD_6                   8404           12.585925  category\n",
      "ICD9_DGNS_CD_5                   5134            7.688736  category\n",
      "ICD9_DGNS_CD_4                   2768            4.145388  category\n",
      "NCH_BENE_IP_DDCTBL_AMT           2178            3.261797   float64\n",
      "ICD9_DGNS_CD_3                   1281            1.918440  category\n",
      "AT_PHYSN_NPI                      673            1.007892   float64\n",
      "ADMTNG_ICD9_DGNS_CD               599            0.897069    object\n",
      "ICD9_DGNS_CD_2                    526            0.787744  category\n",
      "ICD9_DGNS_CD_1                     95            0.142273  category\n",
      "CLM_UTLZTN_DAY_CNT                 68            0.101838   float64\n",
      "discharge_year                     68            0.101838   float64\n",
      "CLM_THRU_DT                        68            0.101838   float64\n",
      "CLM_FROM_DT                        68            0.101838   float64\n",
      "\n",
      "MISSING VALUE CATEGORIES:\n",
      "├── High missing (>50%): 52 columns\n",
      "├── Medium missing (5-50%): 7 columns\n",
      "└── Low missing (≤5%): 11 columns\n"
     ]
    }
   ],
   "source": [
    "def analyze_missing_values(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Comprehensive missing value analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\nMISSING VALUES ANALYSIS - {dataset_name}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "    \n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing_Count': missing_counts,\n",
    "        'Missing_Percentage': missing_pct,\n",
    "        'Data_Type': df.dtypes\n",
    "    }).sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    # Show only columns with missing values\n",
    "    has_missing = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "    \n",
    "    if len(has_missing) > 0:\n",
    "        print(f\"Columns with missing values: {len(has_missing)}\")\n",
    "        print(has_missing)\n",
    "        \n",
    "        # Categorize missing patterns\n",
    "        high_missing = has_missing[has_missing['Missing_Percentage'] > 50]\n",
    "        medium_missing = has_missing[(has_missing['Missing_Percentage'] > 5) & (has_missing['Missing_Percentage'] <= 50)]\n",
    "        low_missing = has_missing[has_missing['Missing_Percentage'] <= 5]\n",
    "        \n",
    "        print(f\"\\nMISSING VALUE CATEGORIES:\")\n",
    "        print(f\"├── High missing (>50%): {len(high_missing)} columns\")\n",
    "        print(f\"├── Medium missing (5-50%): {len(medium_missing)} columns\")\n",
    "        print(f\"└── Low missing (≤5%): {len(low_missing)} columns\")\n",
    "        \n",
    "    else:\n",
    "        print(\"✓ No missing values found!\")\n",
    "    \n",
    "    return missing_summary\n",
    "\n",
    "# Analyze missing values in master beneficiary dataset\n",
    "missing_beneficiary = analyze_missing_values(df_beneficiary_master, \"MASTER BENEFICIARY\")\n",
    "\n",
    "# Analyze missing values in inpatient claims\n",
    "missing_inpatient = analyze_missing_values(df_inpatient, \"INPATIENT CLAIMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c5728",
   "metadata": {},
   "source": [
    "### Block 6: Clean Missing Values in Beneficiary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b7c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING BENEFICIARY MISSING VALUES\n",
      "========================================\n",
      "Death date missing: 338,183 records (expected for living patients)\n",
      "✓ DESYNPUF_ID: No missing values\n",
      "✓ BENE_BIRTH_DT: No missing values\n",
      "✓ BENE_SEX_IDENT_CD: No missing values\n",
      "✓ BENE_RACE_CD: No missing values\n",
      "\n",
      "Cleaning 9 financial columns...\n",
      "\n",
      "Cleaning 4 coverage columns...\n",
      "\n",
      "✓ Beneficiary data cleaning complete\n",
      "  Rows before: 343,644\n",
      "  Rows after: 343,644\n"
     ]
    }
   ],
   "source": [
    "def clean_beneficiary_missing_values(df):\n",
    "    \"\"\"\n",
    "    Clean missing values in beneficiary data with business logic\n",
    "    \"\"\"\n",
    "    print(\"CLEANING BENEFICIARY MISSING VALUES\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    original_rows = len(df_clean)\n",
    "    \n",
    "    # 1. Handle BENE_DEATH_DT (expected to be mostly missing for living patients)\n",
    "    death_missing_before = df_clean['BENE_DEATH_DT'].isnull().sum()\n",
    "    print(f\"Death date missing: {death_missing_before:,} records (expected for living patients)\")\n",
    "    \n",
    "    # 2. Check for any unexpected missing values in key demographic fields\n",
    "    key_demographics = ['DESYNPUF_ID', 'BENE_BIRTH_DT', 'BENE_SEX_IDENT_CD', 'BENE_RACE_CD']\n",
    "    \n",
    "    for col in key_demographics:\n",
    "        if col in df_clean.columns:\n",
    "            missing_count = df_clean[col].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"WARNING: {col} has {missing_count} missing values\")\n",
    "            else:\n",
    "                print(f\"✓ {col}: No missing values\")\n",
    "    \n",
    "    # 3. Handle missing values in financial fields (fill with 0)\n",
    "    financial_cols = [col for col in df_clean.columns if any(x in col for x in ['MEDREIMB', 'BENRES', 'PPPYMT'])]\n",
    "    \n",
    "    if financial_cols:\n",
    "        print(f\"\\nCleaning {len(financial_cols)} financial columns...\")\n",
    "        for col in financial_cols:\n",
    "            missing_before = df_clean[col].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                df_clean[col].fillna(0, inplace=True)\n",
    "                print(f\"  {col}: Filled {missing_before} missing values with 0\")\n",
    "    \n",
    "    # 4. Handle missing values in coverage months (fill with 0)\n",
    "    coverage_cols = [col for col in df_clean.columns if 'CVRAGE' in col or 'CVRG' in col]\n",
    "    \n",
    "    if coverage_cols:\n",
    "        print(f\"\\nCleaning {len(coverage_cols)} coverage columns...\")\n",
    "        for col in coverage_cols:\n",
    "            missing_before = df_clean[col].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                df_clean[col].fillna(0, inplace=True)\n",
    "                print(f\"  {col}: Filled {missing_before} missing values with 0\")\n",
    "    \n",
    "    print(f\"\\n✓ Beneficiary data cleaning complete\")\n",
    "    print(f\"  Rows before: {original_rows:,}\")\n",
    "    print(f\"  Rows after: {len(df_clean):,}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean beneficiary data\n",
    "df_beneficiary_clean = clean_beneficiary_missing_values(df_beneficiary_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0755cd10",
   "metadata": {},
   "source": [
    "### Block 7: Clean Missing Values in Inpatient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3dee488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING INPATIENT MISSING VALUES\n",
      "========================================\n",
      "Handling critical date fields...\n",
      "  WARNING: CLM_FROM_DT has 68 missing values\n",
      "  WARNING: CLM_THRU_DT has 68 missing values\n",
      "\n",
      "Length of stay missing: 68 records\n",
      "\n",
      "Diagnosis codes: 10 columns\n",
      "  Primary diagnosis missing: 95 records\n",
      "    These records may need special handling in analysis\n",
      "\n",
      "Cleaning 4 financial columns...\n",
      "  NCH_BENE_IP_DDCTBL_AMT: Filled 2178 missing values with 0\n",
      "\n",
      "CRITICAL FIELDS FOR READMISSION ANALYSIS:\n",
      "  CLM_ADMSN_DT: 66,773 complete records\n",
      "  NCH_BENE_DSCHRG_DT: 66,773 complete records\n",
      "  DESYNPUF_ID: 66,773 complete records\n",
      "\n",
      "✓ Records usable for readmission analysis: 66,773\n",
      "✓ Inpatient data cleaning complete\n"
     ]
    }
   ],
   "source": [
    "def clean_inpatient_missing_values(df):\n",
    "    \"\"\"\n",
    "    Clean missing values in inpatient claims with healthcare logic\n",
    "    \"\"\"\n",
    "    print(\"CLEANING INPATIENT MISSING VALUES\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    original_rows = len(df_clean)\n",
    "    \n",
    "    # 1. Handle missing admission/discharge dates (critical for readmission analysis)\n",
    "    print(\"Handling critical date fields...\")\n",
    "    \n",
    "    date_fields = ['CLM_FROM_DT', 'CLM_THRU_DT', 'CLM_ADMSN_DT', 'NCH_BENE_DSCHRG_DT']\n",
    "    for field in date_fields:\n",
    "        if field in df_clean.columns:\n",
    "            missing_count = df_clean[field].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"  WARNING: {field} has {missing_count} missing values\")\n",
    "                \n",
    "                # For critical analysis, we may need to drop records with missing key dates\n",
    "                if field in ['CLM_ADMSN_DT', 'NCH_BENE_DSCHRG_DT']:\n",
    "                    print(f\"    These records cannot be used for readmission analysis\")\n",
    "    \n",
    "    # 2. Handle missing length of stay (derive from dates where possible)\n",
    "    if 'CLM_UTLZTN_DAY_CNT' in df_clean.columns:\n",
    "        los_missing = df_clean['CLM_UTLZTN_DAY_CNT'].isnull().sum()\n",
    "        print(f\"\\nLength of stay missing: {los_missing} records\")\n",
    "        \n",
    "        if los_missing > 0 and 'CLM_FROM_DT' in df_clean.columns and 'CLM_THRU_DT' in df_clean.columns:\n",
    "            # Try to calculate LOS from claim dates where both are available\n",
    "            mask = (df_clean['CLM_UTLZTN_DAY_CNT'].isnull() & \n",
    "                   df_clean['CLM_FROM_DT'].notna() & \n",
    "                   df_clean['CLM_THRU_DT'].notna())\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                # Calculate days between dates (YYYYMMDD format)\n",
    "                from_dates = pd.to_datetime(df_clean.loc[mask, 'CLM_FROM_DT'], format='%Y%m%d', errors='coerce')\n",
    "                thru_dates = pd.to_datetime(df_clean.loc[mask, 'CLM_THRU_DT'], format='%Y%m%d', errors='coerce')\n",
    "                calculated_los = (thru_dates - from_dates).dt.days + 1  # +1 for same-day stays\n",
    "                \n",
    "                df_clean.loc[mask, 'CLM_UTLZTN_DAY_CNT'] = calculated_los\n",
    "                print(f\"  Calculated LOS for {calculated_los.notna().sum()} records from claim dates\")\n",
    "    \n",
    "    # 3. Handle missing diagnosis codes (keep as missing - clinically meaningful)\n",
    "    diagnosis_cols = [col for col in df_clean.columns if col.startswith('ICD9_DGNS_CD_')]\n",
    "    if diagnosis_cols:\n",
    "        print(f\"\\nDiagnosis codes: {len(diagnosis_cols)} columns\")\n",
    "        primary_dx_missing = df_clean['ICD9_DGNS_CD_1'].isnull().sum() if 'ICD9_DGNS_CD_1' in df_clean.columns else 0\n",
    "        print(f\"  Primary diagnosis missing: {primary_dx_missing} records\")\n",
    "        if primary_dx_missing > 0:\n",
    "            print(f\"    These records may need special handling in analysis\")\n",
    "    \n",
    "    # 4. Handle missing financial amounts (fill with 0)\n",
    "    financial_cols = [col for col in df_clean.columns if any(x in col for x in ['CLM_PMT_AMT', 'DDCTBL', 'COINSRNC'])]\n",
    "    if financial_cols:\n",
    "        print(f\"\\nCleaning {len(financial_cols)} financial columns...\")\n",
    "        for col in financial_cols:\n",
    "            missing_before = df_clean[col].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                df_clean[col].fillna(0, inplace=True)\n",
    "                print(f\"  {col}: Filled {missing_before} missing values with 0\")\n",
    "    \n",
    "    # 5. Summary of critical missing data for readmission analysis\n",
    "    print(f\"\\nCRITICAL FIELDS FOR READMISSION ANALYSIS:\")\n",
    "    critical_fields = ['CLM_ADMSN_DT', 'NCH_BENE_DSCHRG_DT', 'DESYNPUF_ID']\n",
    "    complete_records = len(df_clean)\n",
    "    \n",
    "    for field in critical_fields:\n",
    "        if field in df_clean.columns:\n",
    "            missing = df_clean[field].isnull().sum()\n",
    "            complete_records = min(complete_records, len(df_clean) - missing)\n",
    "            print(f\"  {field}: {len(df_clean) - missing:,} complete records\")\n",
    "    \n",
    "    print(f\"\\n✓ Records usable for readmission analysis: {complete_records:,}\")\n",
    "    print(f\"✓ Inpatient data cleaning complete\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean inpatient data\n",
    "df_inpatient_clean = clean_inpatient_missing_values(df_inpatient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8f7f5",
   "metadata": {},
   "source": [
    "### Block 8: Data Type Validation and Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8563963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CONSISTENCY VALIDATION\n",
      "========================================\n",
      "BENEFICIARY ID CONSISTENCY:\n",
      "├── Unique beneficiaries in master file: 116,352\n",
      "├── Unique beneficiaries in inpatient: 37,780\n",
      "└── Inpatient IDs found in beneficiary: 37,780\n",
      "✓ All inpatient beneficiaries found in master file\n",
      "\n",
      "DATE VALIDATION:\n",
      "✓ All discharge dates are after admission dates\n",
      "Admission date range: 20071127 to 20101230\n",
      "⚠️  WARNING: 226 records outside 2008-2010 range\n",
      "\n",
      "CHRONIC CONDITION CONSISTENCY:\n",
      "Analyzing 11 chronic conditions...\n",
      "  SP_ALZHDMTA: 29542 beneficiaries with condition reversals\n",
      "  SP_CHF: 33229 beneficiaries with condition reversals\n",
      "  SP_CHRNKIDN: 23227 beneficiaries with condition reversals\n"
     ]
    }
   ],
   "source": [
    "def validate_data_consistency():\n",
    "    \"\"\"\n",
    "    Validate data consistency across combined datasets\n",
    "    \"\"\"\n",
    "    print(\"DATA CONSISTENCY VALIDATION\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # 1. Check beneficiary ID consistency\n",
    "    beneficiary_ids = set(df_beneficiary_clean['DESYNPUF_ID'])\n",
    "    inpatient_ids = set(df_inpatient_clean['DESYNPUF_ID'])\n",
    "    \n",
    "    print(\"BENEFICIARY ID CONSISTENCY:\")\n",
    "    print(f\"├── Unique beneficiaries in master file: {len(beneficiary_ids):,}\")\n",
    "    print(f\"├── Unique beneficiaries in inpatient: {len(inpatient_ids):,}\")\n",
    "    print(f\"└── Inpatient IDs found in beneficiary: {len(inpatient_ids & beneficiary_ids):,}\")\n",
    "    \n",
    "    orphaned_inpatient = inpatient_ids - beneficiary_ids\n",
    "    if orphaned_inpatient:\n",
    "        print(f\"⚠️  WARNING: {len(orphaned_inpatient)} inpatient beneficiaries not in master file\")\n",
    "    else:\n",
    "        print(\"✓ All inpatient beneficiaries found in master file\")\n",
    "    \n",
    "    # 2. Validate date formats and ranges\n",
    "    print(f\"\\nDATE VALIDATION:\")\n",
    "    \n",
    "    # Check admission/discharge date consistency in inpatient data\n",
    "    if 'CLM_ADMSN_DT' in df_inpatient_clean.columns and 'NCH_BENE_DSCHRG_DT' in df_inpatient_clean.columns:\n",
    "        # Find records where discharge is before admission\n",
    "        invalid_dates = df_inpatient_clean[\n",
    "            (df_inpatient_clean['CLM_ADMSN_DT'].notna()) & \n",
    "            (df_inpatient_clean['NCH_BENE_DSCHRG_DT'].notna()) &\n",
    "            (df_inpatient_clean['NCH_BENE_DSCHRG_DT'] < df_inpatient_clean['CLM_ADMSN_DT'])\n",
    "        ]\n",
    "        \n",
    "        if len(invalid_dates) > 0:\n",
    "            print(f\"⚠️  WARNING: {len(invalid_dates)} records with discharge before admission\")\n",
    "        else:\n",
    "            print(\"✓ All discharge dates are after admission dates\")\n",
    "    \n",
    "    # 3. Check for reasonable date ranges (2008-2010)\n",
    "    if 'CLM_ADMSN_DT' in df_inpatient_clean.columns:\n",
    "        min_date = df_inpatient_clean['CLM_ADMSN_DT'].min()\n",
    "        max_date = df_inpatient_clean['CLM_ADMSN_DT'].max()\n",
    "        print(f\"Admission date range: {min_date} to {max_date}\")\n",
    "        \n",
    "        # Check for dates outside expected range\n",
    "        outside_range = df_inpatient_clean[\n",
    "            (df_inpatient_clean['CLM_ADMSN_DT'] < 20080101) | \n",
    "            (df_inpatient_clean['CLM_ADMSN_DT'] > 20101231)\n",
    "        ]\n",
    "        \n",
    "        if len(outside_range) > 0:\n",
    "            print(f\"⚠️  WARNING: {len(outside_range)} records outside 2008-2010 range\")\n",
    "        else:\n",
    "            print(\"✓ All dates within expected range (2008-2010)\")\n",
    "    \n",
    "    # 4. Validate chronic condition consistency across years\n",
    "    print(f\"\\nCHRONIC CONDITION CONSISTENCY:\")\n",
    "    chronic_conditions = [col for col in df_beneficiary_clean.columns if col.startswith('SP_') and col != 'SP_STATE_CODE']\n",
    "    \n",
    "    if chronic_conditions:\n",
    "        print(f\"Analyzing {len(chronic_conditions)} chronic conditions...\")\n",
    "        \n",
    "        # Check for beneficiaries where chronic conditions \"disappear\" (should be rare)\n",
    "        condition_changes = {}\n",
    "        \n",
    "        for condition in chronic_conditions[:3]:  # Check first 3 as example\n",
    "            # Group by beneficiary and check if condition ever goes from True to False\n",
    "            ben_condition = df_beneficiary_clean.groupby('DESYNPUF_ID')[condition].apply(list)\n",
    "            \n",
    "            reversals = 0\n",
    "            for ben_id, values in ben_condition.items():\n",
    "                if len(values) > 1:\n",
    "                    # Check if any True is followed by False\n",
    "                    for i in range(len(values)-1):\n",
    "                        if values[i] == True and values[i+1] == False:\n",
    "                            reversals += 1\n",
    "                            break\n",
    "            \n",
    "            condition_changes[condition] = reversals\n",
    "            print(f\"  {condition}: {reversals} beneficiaries with condition reversals\")\n",
    "\n",
    "validate_data_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728027b",
   "metadata": {},
   "source": [
    "### Block 9: Create Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56009e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY REPORT\n",
      "==================================================\n",
      "MASTER BENEFICIARY DATASET:\n",
      "├── Total records: 343,644\n",
      "├── Unique beneficiaries: 116,352\n",
      "├── Years covered: [np.int64(2008), np.int64(2009), np.int64(2010)]\n",
      "└── Memory usage: 80.3 MB\n",
      "\n",
      "INPATIENT CLAIMS DATASET:\n",
      "├── Total claims: 66,773\n",
      "├── Unique beneficiaries: 37,780\n",
      "├── Date range: 20071127 to 20101231\n",
      "└── Memory usage: 44.0 MB\n",
      "\n",
      "DATA COMPLETENESS FOR KEY FIELDS:\n",
      "├── Beneficiary DESYNPUF_ID: 100.0% complete\n",
      "├── Beneficiary BENE_BIRTH_DT: 100.0% complete\n",
      "├── Beneficiary BENE_SEX_IDENT_CD: 100.0% complete\n",
      "├── Beneficiary BENE_RACE_CD: 100.0% complete\n",
      "├── Inpatient DESYNPUF_ID: 100.0% complete\n",
      "├── Inpatient CLM_ADMSN_DT: 100.0% complete\n",
      "├── Inpatient NCH_BENE_DSCHRG_DT: 100.0% complete\n",
      "├── Inpatient ICD9_DGNS_CD_1: 99.9% complete\n",
      "\n",
      "READINESS FOR READMISSION ANALYSIS:\n",
      "├── Complete inpatient records: 66,773 (100.0%)\n",
      "├── Beneficiaries with complete data: 37,780\n",
      "└── Ready for target variable creation: ✓\n"
     ]
    }
   ],
   "source": [
    "def create_data_quality_report():\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality report\n",
    "    \"\"\"\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Master dataset statistics\n",
    "    print(\"MASTER BENEFICIARY DATASET:\")\n",
    "    print(f\"├── Total records: {len(df_beneficiary_clean):,}\")\n",
    "    print(f\"├── Unique beneficiaries: {df_beneficiary_clean['DESYNPUF_ID'].nunique():,}\")\n",
    "    print(f\"├── Years covered: {sorted(df_beneficiary_clean['BENE_YEAR'].unique())}\")\n",
    "    print(f\"└── Memory usage: {df_beneficiary_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Inpatient dataset statistics  \n",
    "    print(f\"\\nINPATIENT CLAIMS DATASET:\")\n",
    "    print(f\"├── Total claims: {len(df_inpatient_clean):,}\")\n",
    "    print(f\"├── Unique beneficiaries: {df_inpatient_clean['DESYNPUF_ID'].nunique():,}\")\n",
    "    print(f\"├── Date range: {df_inpatient_clean['CLM_ADMSN_DT'].min()} to {df_inpatient_clean['NCH_BENE_DSCHRG_DT'].max()}\")\n",
    "    print(f\"└── Memory usage: {df_inpatient_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Data completeness for key fields\n",
    "    print(f\"\\nDATA COMPLETENESS FOR KEY FIELDS:\")\n",
    "    \n",
    "    # Beneficiary key fields\n",
    "    ben_key_fields = ['DESYNPUF_ID', 'BENE_BIRTH_DT', 'BENE_SEX_IDENT_CD', 'BENE_RACE_CD']\n",
    "    for field in ben_key_fields:\n",
    "        if field in df_beneficiary_clean.columns:\n",
    "            completeness = (1 - df_beneficiary_clean[field].isnull().mean()) * 100\n",
    "            print(f\"├── Beneficiary {field}: {completeness:.1f}% complete\")\n",
    "    \n",
    "    # Inpatient key fields\n",
    "    inp_key_fields = ['DESYNPUF_ID', 'CLM_ADMSN_DT', 'NCH_BENE_DSCHRG_DT', 'ICD9_DGNS_CD_1']\n",
    "    for field in inp_key_fields:\n",
    "        if field in df_inpatient_clean.columns:\n",
    "            completeness = (1 - df_inpatient_clean[field].isnull().mean()) * 100\n",
    "            print(f\"├── Inpatient {field}: {completeness:.1f}% complete\")\n",
    "    \n",
    "    # Records ready for analysis\n",
    "    print(f\"\\nREADINESS FOR READMISSION ANALYSIS:\")\n",
    "    \n",
    "    # Count records with all required fields for readmission analysis\n",
    "    required_fields = ['DESYNPUF_ID', 'CLM_ADMSN_DT', 'NCH_BENE_DSCHRG_DT']\n",
    "    complete_mask = df_inpatient_clean[required_fields].notna().all(axis=1)\n",
    "    complete_records = complete_mask.sum()\n",
    "    \n",
    "    print(f\"├── Complete inpatient records: {complete_records:,} ({complete_records/len(df_inpatient_clean)*100:.1f}%)\")\n",
    "    print(f\"├── Beneficiaries with complete data: {df_inpatient_clean[complete_mask]['DESYNPUF_ID'].nunique():,}\")\n",
    "    print(f\"└── Ready for target variable creation: ✓\")\n",
    "    \n",
    "    return complete_records\n",
    "\n",
    "complete_records = create_data_quality_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c22fbc",
   "metadata": {},
   "source": [
    "### Block 10: Save Master Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dbed383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING MASTER DATASETS\n",
      "==============================\n",
      "✓ Beneficiary master dataset saved:\n",
      "  ├── CSV: data/processed/beneficiary_master_clean.csv\n",
      "  └── Parquet: data/processed/beneficiary_master_clean.parquet\n",
      "✓ Inpatient master dataset saved:\n",
      "  ├── CSV: data/processed/inpatient_master_clean.csv\n",
      "  └── Parquet: data/processed/inpatient_master_clean.parquet\n",
      "✓ Summary saved: data/processed/data_combination_summary.txt\n"
     ]
    }
   ],
   "source": [
    "def save_master_datasets():\n",
    "    \"\"\"\n",
    "    Save cleaned and combined master datasets\n",
    "    \"\"\"\n",
    "    print(\"SAVING MASTER DATASETS\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Create directory if needed\n",
    "    os.makedirs(processed_path, exist_ok=True)\n",
    "    \n",
    "    # Save master beneficiary dataset\n",
    "    ben_file_csv = f\"{processed_path}beneficiary_master_clean.csv\"\n",
    "    ben_file_parquet = f\"{processed_path}beneficiary_master_clean.parquet\"\n",
    "    \n",
    "    df_beneficiary_clean.to_csv(ben_file_csv, index=False)\n",
    "    df_beneficiary_clean.to_parquet(ben_file_parquet)\n",
    "    \n",
    "    print(f\"✓ Beneficiary master dataset saved:\")\n",
    "    print(f\"  ├── CSV: {ben_file_csv}\")\n",
    "    print(f\"  └── Parquet: {ben_file_parquet}\")\n",
    "    \n",
    "    # Save clean inpatient dataset\n",
    "    inp_file_csv = f\"{processed_path}inpatient_master_clean.csv\"\n",
    "    inp_file_parquet = f\"{processed_path}inpatient_master_clean.parquet\"\n",
    "    \n",
    "    df_inpatient_clean.to_csv(inp_file_csv, index=False)\n",
    "    df_inpatient_clean.to_parquet(inp_file_parquet)\n",
    "    \n",
    "    print(f\"✓ Inpatient master dataset saved:\")\n",
    "    print(f\"  ├── CSV: {inp_file_csv}\")\n",
    "    print(f\"  └── Parquet: {inp_file_parquet}\")\n",
    "    \n",
    "    # Save data summary\n",
    "    summary_file = f\"{processed_path}data_combination_summary.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"DATA COMBINATION & PREPROCESSING SUMMARY\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        f.write(f\"Master Beneficiary Dataset:\\n\")\n",
    "        f.write(f\"- Total records: {len(df_beneficiary_clean):,}\\n\")\n",
    "        f.write(f\"- Unique beneficiaries: {df_beneficiary_clean['DESYNPUF_ID'].nunique():,}\\n\")\n",
    "        f.write(f\"- Years: {sorted(df_beneficiary_clean['BENE_YEAR'].unique())}\\n\\n\")\n",
    "        f.write(f\"Inpatient Claims Dataset:\\n\")\n",
    "        f.write(f\"- Total claims: {len(df_inpatient_clean):,}\\n\")\n",
    "        f.write(f\"- Unique beneficiaries: {df_inpatient_clean['DESYNPUF_ID'].nunique():,}\\n\")\n",
    "        f.write(f\"- Complete records: {complete_records:,}\\n\\n\")\n",
    "        f.write(f\"Ready for Phase C: Target Variable Creation\\n\")\n",
    "    \n",
    "    print(f\"✓ Summary saved: {summary_file}\")\n",
    "\n",
    "save_master_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2c9bd",
   "metadata": {},
   "source": [
    "### Block 11: Phase B Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e4e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE B COMPLETE: DATA COMBINATION & PREPROCESSING\n",
      "============================================================\n",
      "✅ ACCOMPLISHMENTS:\n",
      "├── Combined 3 beneficiary files into master dataset\n",
      "├── Cleaned missing values with healthcare business logic\n",
      "├── Validated data consistency across files\n",
      "├── Optimized data types and memory usage\n",
      "└── Created analysis-ready datasets\n",
      "\n",
      "📊 FINAL DATASET STATISTICS:\n",
      "├── Master Beneficiaries: 343,644 records\n",
      "├── Unique Beneficiaries: 116,352\n",
      "├── Inpatient Claims: 66,773 claims\n",
      "├── Claims with Complete Data: 66,773\n",
      "└── Beneficiaries with Claims: 37,780\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "├── Ready for Notebook 3: Target Variable Creation\n",
      "├── Define 30-day readmission events\n",
      "├── Create readmission target variable\n",
      "├── Calculate baseline readmission rates\n",
      "└── Validate business logic\n",
      "\n",
      "✓ Phase B Complete - Ready for Target Variable Creation!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE B COMPLETE: DATA COMBINATION & PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"✅ ACCOMPLISHMENTS:\")\n",
    "print(\"├── Combined 3 beneficiary files into master dataset\")\n",
    "print(\"├── Cleaned missing values with healthcare business logic\")\n",
    "print(\"├── Validated data consistency across files\")\n",
    "print(\"├── Optimized data types and memory usage\")\n",
    "print(\"└── Created analysis-ready datasets\")\n",
    "\n",
    "print(f\"\\n📊 FINAL DATASET STATISTICS:\")\n",
    "print(f\"├── Master Beneficiaries: {len(df_beneficiary_clean):,} records\")\n",
    "print(f\"├── Unique Beneficiaries: {df_beneficiary_clean['DESYNPUF_ID'].nunique():,}\")\n",
    "print(f\"├── Inpatient Claims: {len(df_inpatient_clean):,} claims\")\n",
    "print(f\"├── Claims with Complete Data: {complete_records:,}\")\n",
    "print(f\"└── Beneficiaries with Claims: {df_inpatient_clean['DESYNPUF_ID'].nunique():,}\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"├── Ready for Notebook 3: Target Variable Creation\")\n",
    "print(\"├── Define 30-day readmission events\")\n",
    "print(\"├── Create readmission target variable\")  \n",
    "print(\"├── Calculate baseline readmission rates\")\n",
    "print(\"└── Validate business logic\")\n",
    "\n",
    "print(f\"\\n✓ Phase B Complete - Ready for Target Variable Creation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
