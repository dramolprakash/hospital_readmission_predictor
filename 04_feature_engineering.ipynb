{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7326235a",
   "metadata": {},
   "source": [
    "### Block 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e69b5e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "Pandas version: 2.2.3\n",
      "NumPy version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c07e7",
   "metadata": {},
   "source": [
    "### Block 2: Load Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f2f3aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target dataset from Phase C...\n",
      "✓ Datasets loaded successfully!\n",
      "├── Target dataset: 66,773 records\n",
      "├── Features available: 46 columns\n",
      "├── Inpatient claims: 66,773 records\n",
      "└── Target variable: READMISSION_30DAY (10.13% positive)\n"
     ]
    }
   ],
   "source": [
    "# Load target dataset from Phase C\n",
    "import os\n",
    "\n",
    "features_path = \"data/features/\"\n",
    "processed_path = \"data/processed/\"\n",
    "\n",
    "print(\"Loading target dataset from Phase C...\")\n",
    "\n",
    "# Load main target dataset\n",
    "df_target = pd.read_parquet(f\"{features_path}readmission_target_dataset.parquet\")\n",
    "\n",
    "# Load clean inpatient data for historical features\n",
    "df_inpatient = pd.read_parquet(f\"{processed_path}inpatient_master_clean.parquet\")\n",
    "\n",
    "print(\"✓ Datasets loaded successfully!\")\n",
    "print(f\"├── Target dataset: {len(df_target):,} records\")\n",
    "print(f\"├── Features available: {len(df_target.columns)} columns\")\n",
    "print(f\"├── Inpatient claims: {len(df_inpatient):,} records\")\n",
    "print(f\"└── Target variable: READMISSION_30DAY ({df_target['READMISSION_30DAY'].mean()*100:.2f}% positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf1fe6c",
   "metadata": {},
   "source": [
    "### Block 3: Baseline Feature Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c2055f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE FEATURE ASSESSMENT\n",
      "========================================\n",
      "FEATURE CATEGORIES:\n",
      "├── Demographic features: 9\n",
      "├── Clinical features: 6\n",
      "├── Chronic conditions: 11\n",
      "├── Temporal features: 14\n",
      "└── Total features: 46\n",
      "\n",
      "DATA QUALITY ASSESSMENT:\n",
      "├── Complete records: 23\n",
      "├── Features with >10% missing: 2\n",
      "  High missing features:\n",
      "    - DAYS_TO_READMISSION: 89.9% missing\n",
      "    - READMISSION_DATE: 89.9% missing\n",
      "\n",
      "KEY FEATURE COMPLETENESS:\n",
      "├── AGE_AT_ADMISSION: 99.7% complete\n",
      "├── BENE_SEX_IDENT_CD: 99.7% complete\n",
      "├── LOS_CALCULATED: 100.0% complete\n",
      "├── CHRONIC_CONDITION_COUNT: 100.0% complete\n"
     ]
    }
   ],
   "source": [
    "def assess_baseline_features(df):\n",
    "    \"\"\"\n",
    "    Assess the quality and completeness of baseline features\n",
    "    \"\"\"\n",
    "    print(\"BASELINE FEATURE ASSESSMENT\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Categorize existing features\n",
    "    demographic_features = [col for col in df.columns if any(x in col for x in \n",
    "                           ['AGE', 'SEX', 'RACE', 'STATE', 'BENE_'])]\n",
    "    \n",
    "    clinical_features = [col for col in df.columns if any(x in col for x in \n",
    "                        ['LOS', 'DRG', 'ICD9', 'CLM_'])]\n",
    "    \n",
    "    chronic_conditions = [col for col in df.columns if col.startswith('SP_') and col != 'SP_STATE_CODE']\n",
    "    \n",
    "    temporal_features = [col for col in df.columns if any(x in col for x in \n",
    "                        ['ADMISSION', 'DISCHARGE', 'SEASON', 'WEEKEND'])]\n",
    "    \n",
    "    print(f\"FEATURE CATEGORIES:\")\n",
    "    print(f\"├── Demographic features: {len(demographic_features)}\")\n",
    "    print(f\"├── Clinical features: {len(clinical_features)}\")\n",
    "    print(f\"├── Chronic conditions: {len(chronic_conditions)}\")\n",
    "    print(f\"├── Temporal features: {len(temporal_features)}\")\n",
    "    print(f\"└── Total features: {len(df.columns)}\")\n",
    "    \n",
    "    # Check data quality\n",
    "    print(f\"\\nDATA QUALITY ASSESSMENT:\")\n",
    "    missing_counts = df.isnull().sum()\n",
    "    high_missing = missing_counts[missing_counts > len(df) * 0.1]  # >10% missing\n",
    "    \n",
    "    print(f\"├── Complete records: {(missing_counts == 0).sum()}\")\n",
    "    print(f\"├── Features with >10% missing: {len(high_missing)}\")\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"  High missing features:\")\n",
    "        for feature, count in high_missing.items():\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\"    - {feature}: {pct:.1f}% missing\")\n",
    "    \n",
    "    # Key feature completeness\n",
    "    key_features = ['AGE_AT_ADMISSION', 'BENE_SEX_IDENT_CD', 'LOS_CALCULATED', \n",
    "                   'CHRONIC_CONDITION_COUNT']\n",
    "    \n",
    "    print(f\"\\nKEY FEATURE COMPLETENESS:\")\n",
    "    for feature in key_features:\n",
    "        if feature in df.columns:\n",
    "            completeness = (1 - df[feature].isnull().mean()) * 100\n",
    "            print(f\"├── {feature}: {completeness:.1f}% complete\")\n",
    "    \n",
    "    return {\n",
    "        'demographic': demographic_features,\n",
    "        'clinical': clinical_features, \n",
    "        'chronic': chronic_conditions,\n",
    "        'temporal': temporal_features\n",
    "    }\n",
    "\n",
    "# Assess baseline features\n",
    "baseline_features = assess_baseline_features(df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8607e4",
   "metadata": {},
   "source": [
    "### checking block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97726b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIAGNOSING BENE_ESRD_IND COLUMN\n",
      "===================================\n",
      "Column data type: object\n",
      "Unique values: [False True None]\n",
      "Value counts:\n",
      "BENE_ESRD_IND\n",
      "False    49707\n",
      "True     16840\n",
      "None       226\n",
      "Name: count, dtype: int64\n",
      "Null count: 226\n",
      "Non-null count: 66547\n",
      "None values: 0\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC CODE - Run this first to understand the ESRD column\n",
    "print(\"DIAGNOSING BENE_ESRD_IND COLUMN\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "if 'BENE_ESRD_IND' in df_target.columns:\n",
    "    print(f\"Column data type: {df_target['BENE_ESRD_IND'].dtype}\")\n",
    "    print(f\"Unique values: {df_target['BENE_ESRD_IND'].unique()}\")\n",
    "    print(f\"Value counts:\")\n",
    "    print(df_target['BENE_ESRD_IND'].value_counts(dropna=False))\n",
    "    print(f\"Null count: {df_target['BENE_ESRD_IND'].isnull().sum()}\")\n",
    "    print(f\"Non-null count: {df_target['BENE_ESRD_IND'].notna().sum()}\")\n",
    "    \n",
    "    # Check for None values specifically\n",
    "    none_count = (df_target['BENE_ESRD_IND'] == None).sum()\n",
    "    print(f\"None values: {none_count}\")\n",
    "else:\n",
    "    print(\"BENE_ESRD_IND column not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199c05b",
   "metadata": {},
   "source": [
    "### checking Age coloum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "724be0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIAGNOSING AGE_AT_ADMISSION COLUMN\n",
      "========================================\n",
      "Column exists: ✓\n",
      "Data type: float64\n",
      "Non-null count: 66,547\n",
      "Null count: 226\n",
      "Min age: 24.58590006844627\n",
      "Max age: 101.71115674195757\n",
      "Mean age: 73.8\n",
      "Sample values: [86.86379192334017, 66.27789185489391, 66.66392881587953, 66.71047227926078, 67.48254620123203]\n",
      "Ages in reasonable range (0-120): 66,547\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSE AGE COLUMN\n",
    "print(\"DIAGNOSING AGE_AT_ADMISSION COLUMN\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if 'AGE_AT_ADMISSION' in df_target.columns:\n",
    "    print(f\"Column exists: ✓\")\n",
    "    print(f\"Data type: {df_target['AGE_AT_ADMISSION'].dtype}\")\n",
    "    print(f\"Non-null count: {df_target['AGE_AT_ADMISSION'].notna().sum():,}\")\n",
    "    print(f\"Null count: {df_target['AGE_AT_ADMISSION'].isnull().sum():,}\")\n",
    "    \n",
    "    if df_target['AGE_AT_ADMISSION'].notna().sum() > 0:\n",
    "        print(f\"Min age: {df_target['AGE_AT_ADMISSION'].min()}\")\n",
    "        print(f\"Max age: {df_target['AGE_AT_ADMISSION'].max()}\")\n",
    "        print(f\"Mean age: {df_target['AGE_AT_ADMISSION'].mean():.1f}\")\n",
    "        print(f\"Sample values: {df_target['AGE_AT_ADMISSION'].dropna().head().tolist()}\")\n",
    "    else:\n",
    "        print(\"All age values are null!\")\n",
    "        \n",
    "    # Check if ages are in reasonable range\n",
    "    if df_target['AGE_AT_ADMISSION'].notna().sum() > 0:\n",
    "        reasonable_ages = df_target['AGE_AT_ADMISSION'].between(0, 120)\n",
    "        print(f\"Ages in reasonable range (0-120): {reasonable_ages.sum():,}\")\n",
    "else:\n",
    "    print(\"AGE_AT_ADMISSION column not found!\")\n",
    "    print(\"Available columns with 'AGE' in name:\")\n",
    "    age_cols = [col for col in df_target.columns if 'AGE' in col.upper()]\n",
    "    print(age_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39471da0",
   "metadata": {},
   "source": [
    "### Block 4: Enhanced Demographic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b180ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING ENHANCED DEMOGRAPHIC FEATURES\n",
      "=============================================\n",
      "Creating age groups...\n",
      "  ✓ Age groups created\n",
      "    Age distribution:\n",
      "      Under_65: 11,596 (17.4%)\n",
      "      65-69: 10,468 (15.7%)\n",
      "      70-74: 11,250 (16.8%)\n",
      "      75-79: 11,066 (16.6%)\n",
      "      80-84: 9,889 (14.8%)\n",
      "      85_Plus: 12,278 (18.4%)\n",
      "\n",
      "Creating gender features...\n",
      "  ✓ Gender distribution:\n",
      "      Female: 37,622 (56.3%)\n",
      "      Male: 28,925 (43.3%)\n",
      "\n",
      "Creating race/ethnicity features...\n",
      "  ✓ Race/ethnicity distribution:\n",
      "      White: 56,186 (84.1%)\n",
      "      Black: 6,978 (10.5%)\n",
      "      Other: 2,087 (3.1%)\n",
      "      Hispanic: 1,296 (1.9%)\n",
      "\n",
      "Creating geographic features...\n",
      "  ✓ Geographic features created\n",
      "    Top 5 states: [5.0, 10.0, 45.0, 33.0, 14.0]\n",
      "\n",
      "Processing ESRD indicator...\n",
      "  Original ESRD values: BENE_ESRD_IND\n",
      "False    49707\n",
      "True     16840\n",
      "None       226\n",
      "Name: count, dtype: int64\n",
      "  ✓ ESRD processing complete:\n",
      "    - True values converted to 1: 16,840\n",
      "    - False/None values converted to 0: 49,933\n",
      "    - ESRD prevalence: 25.2%\n",
      "  ✓ Verification - HAS_ESRD values: HAS_ESRD\n",
      "0    49933\n",
      "1    16840\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Enhanced demographic features created\n"
     ]
    }
   ],
   "source": [
    "def create_demographic_features(df):\n",
    "    \"\"\"\n",
    "    Create enhanced demographic features for modeling\n",
    "    \"\"\"\n",
    "    print(\"CREATING ENHANCED DEMOGRAPHIC FEATURES\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # 1. Age group categorization\n",
    "    if 'AGE_AT_ADMISSION' in df_enhanced.columns:\n",
    "        print(\"Creating age groups...\")\n",
    "        \n",
    "        # Medicare-relevant age groups\n",
    "        df_enhanced['AGE_GROUP'] = pd.cut(\n",
    "            df_enhanced['AGE_AT_ADMISSION'],\n",
    "            bins=[0, 65, 70, 75, 80, 85, 120],\n",
    "            labels=['Under_65', '65-69', '70-74', '75-79', '80-84', '85_Plus'],\n",
    "            right=False\n",
    "        )\n",
    "        \n",
    "        # Age categories for analysis\n",
    "        df_enhanced['AGE_CATEGORY'] = pd.cut(\n",
    "            df_enhanced['AGE_AT_ADMISSION'],\n",
    "            bins=[0, 65, 75, 85, 120],\n",
    "            labels=['Under_65', 'Young_Senior', 'Old_Senior', 'Very_Old'],\n",
    "            right=False\n",
    "        )\n",
    "        \n",
    "        # High-risk age flag (80+)\n",
    "        df_enhanced['HIGH_RISK_AGE'] = (df_enhanced['AGE_AT_ADMISSION'] >= 80).astype(int)\n",
    "        \n",
    "        print(f\"  ✓ Age groups created\")\n",
    "        print(f\"    Age distribution:\")\n",
    "        age_dist = df_enhanced['AGE_GROUP'].value_counts().sort_index()\n",
    "        for group, count in age_dist.items():\n",
    "            pct = count / len(df_enhanced) * 100\n",
    "            print(f\"      {group}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # 2. Gender encoding\n",
    "    if 'BENE_SEX_IDENT_CD' in df_enhanced.columns:\n",
    "        print(\"\\nCreating gender features...\")\n",
    "        df_enhanced['GENDER'] = df_enhanced['BENE_SEX_IDENT_CD'].map({1: 'Male', 2: 'Female'})\n",
    "        df_enhanced['IS_MALE'] = (df_enhanced['BENE_SEX_IDENT_CD'] == 1).astype(int)\n",
    "        df_enhanced['IS_FEMALE'] = (df_enhanced['BENE_SEX_IDENT_CD'] == 2).astype(int)\n",
    "        \n",
    "        gender_dist = df_enhanced['GENDER'].value_counts()\n",
    "        print(f\"  ✓ Gender distribution:\")\n",
    "        for gender, count in gender_dist.items():\n",
    "            pct = count / len(df_enhanced) * 100\n",
    "            print(f\"      {gender}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # 3. Race/Ethnicity grouping\n",
    "    if 'BENE_RACE_CD' in df_enhanced.columns:\n",
    "        print(\"\\nCreating race/ethnicity features...\")\n",
    "        \n",
    "        # CMS race codes: 1=White, 2=Black, 3=Other, 5=Hispanic\n",
    "        race_mapping = {1: 'White', 2: 'Black', 3: 'Other', 5: 'Hispanic'}\n",
    "        df_enhanced['RACE_ETHNICITY'] = df_enhanced['BENE_RACE_CD'].map(race_mapping)\n",
    "        \n",
    "        # Binary indicators for modeling\n",
    "        df_enhanced['IS_WHITE'] = (df_enhanced['BENE_RACE_CD'] == 1).astype(int)\n",
    "        df_enhanced['IS_BLACK'] = (df_enhanced['BENE_RACE_CD'] == 2).astype(int)\n",
    "        df_enhanced['IS_HISPANIC'] = (df_enhanced['BENE_RACE_CD'] == 5).astype(int)\n",
    "        df_enhanced['IS_MINORITY'] = (df_enhanced['BENE_RACE_CD'] != 1).astype(int)\n",
    "        \n",
    "        race_dist = df_enhanced['RACE_ETHNICITY'].value_counts()\n",
    "        print(f\"  ✓ Race/ethnicity distribution:\")\n",
    "        for race, count in race_dist.items():\n",
    "            pct = count / len(df_enhanced) * 100\n",
    "            print(f\"      {race}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # 4. Geographic features\n",
    "    if 'SP_STATE_CODE' in df_enhanced.columns:\n",
    "        print(\"\\nCreating geographic features...\")\n",
    "        \n",
    "        # State frequency encoding (states with high Medicare utilization)\n",
    "        state_counts = df_enhanced['SP_STATE_CODE'].value_counts()\n",
    "        df_enhanced['STATE_FREQUENCY'] = df_enhanced['SP_STATE_CODE'].map(state_counts)\n",
    "        \n",
    "        # High-volume states (top 10)\n",
    "        top_states = state_counts.head(10).index\n",
    "        df_enhanced['HIGH_VOLUME_STATE'] = df_enhanced['SP_STATE_CODE'].isin(top_states).astype(int)\n",
    "        \n",
    "        print(f\"  ✓ Geographic features created\")\n",
    "        print(f\"    Top 5 states: {list(state_counts.head().index)}\")\n",
    "    \n",
    "    # 5. ESRD (End-Stage Renal Disease) indicator - SPECIFIC FIX FOR YOUR DATA\n",
    "    if 'BENE_ESRD_IND' in df_enhanced.columns:\n",
    "        print(\"\\nProcessing ESRD indicator...\")\n",
    "        print(f\"  Original ESRD values: {df_enhanced['BENE_ESRD_IND'].value_counts(dropna=False)}\")\n",
    "        \n",
    "        # Handle the specific case: False, True, None as object type\n",
    "        # Convert None to False, keep True as True, keep False as False\n",
    "        df_enhanced['HAS_ESRD'] = df_enhanced['BENE_ESRD_IND'].fillna(False)  # Convert None to False\n",
    "        df_enhanced['HAS_ESRD'] = (df_enhanced['HAS_ESRD'] == True).astype(int)  # Convert True to 1, False to 0\n",
    "        \n",
    "        esrd_count = df_enhanced['HAS_ESRD'].sum()\n",
    "        esrd_pct = esrd_count / len(df_enhanced) * 100\n",
    "        print(f\"  ✓ ESRD processing complete:\")\n",
    "        print(f\"    - True values converted to 1: {esrd_count:,}\")\n",
    "        print(f\"    - False/None values converted to 0: {len(df_enhanced) - esrd_count:,}\")\n",
    "        print(f\"    - ESRD prevalence: {esrd_pct:.1f}%\")\n",
    "        \n",
    "        # Verify the conversion worked\n",
    "        print(f\"  ✓ Verification - HAS_ESRD values: {df_enhanced['HAS_ESRD'].value_counts()}\")\n",
    "    \n",
    "    print(f\"\\n✓ Enhanced demographic features created\")\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Create enhanced demographic features\n",
    "df_with_demographics = create_demographic_features(df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e2c08",
   "metadata": {},
   "source": [
    "### Block 5: Advanced Clinical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8557c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING ADVANCED CLINICAL FEATURES\n",
      "========================================\n",
      "Creating length of stay features...\n",
      "  ✓ LOS features created\n",
      "    Medium_4-7: 35,377 (53.0%)\n",
      "    Long_8-14: 16,528 (24.8%)\n",
      "    Short_2-3: 8,661 (13.0%)\n",
      "    Very_Long_15-30: 5,181 (7.8%)\n",
      "    Extended_30+: 1,026 (1.5%)\n",
      "    Same_Day: 0 (0.0%)\n",
      "\n",
      "Creating DRG features...\n",
      "  ✓ DRG features created\n",
      "    Unique DRGs: 739\n",
      "    Common DRGs (≥100 cases): 243\n",
      "    High-risk DRGs: 185\n",
      "\n",
      "Creating primary diagnosis features...\n",
      "  ✓ Primary diagnosis features created\n",
      "    Unique primary diagnoses: 2740\n",
      "    High-readmission diagnoses: 13,841\n",
      "\n",
      "Enhancing admission timing features...\n",
      "  Weekend readmission rate: 9.8%\n",
      "  Weekday readmission rate: 10.3%\n",
      "\n",
      "✓ Advanced clinical features created\n"
     ]
    }
   ],
   "source": [
    "def create_clinical_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced clinical features from admission data\n",
    "    \"\"\"\n",
    "    print(\"CREATING ADVANCED CLINICAL FEATURES\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    df_clinical = df.copy()\n",
    "    \n",
    "    # 1. Enhanced Length of Stay features\n",
    "    if 'LOS_CALCULATED' in df_clinical.columns:\n",
    "        print(\"Creating length of stay features...\")\n",
    "        \n",
    "        # LOS categories\n",
    "        df_clinical['LOS_CATEGORY'] = pd.cut(\n",
    "            df_clinical['LOS_CALCULATED'],\n",
    "            bins=[0, 1, 3, 7, 14, 30, 999],\n",
    "            labels=['Same_Day', 'Short_2-3', 'Medium_4-7', 'Long_8-14', 'Very_Long_15-30', 'Extended_30+'],\n",
    "            right=False\n",
    "        )\n",
    "        \n",
    "        # LOS risk indicators\n",
    "        df_clinical['SHORT_STAY'] = (df_clinical['LOS_CALCULATED'] <= 2).astype(int)\n",
    "        df_clinical['LONG_STAY'] = (df_clinical['LOS_CALCULATED'] >= 7).astype(int)\n",
    "        df_clinical['VERY_LONG_STAY'] = (df_clinical['LOS_CALCULATED'] >= 14).astype(int)\n",
    "        \n",
    "        # LOS percentile within dataset\n",
    "        df_clinical['LOS_PERCENTILE'] = df_clinical['LOS_CALCULATED'].rank(pct=True)\n",
    "        \n",
    "        print(f\"  ✓ LOS features created\")\n",
    "        los_dist = df_clinical['LOS_CATEGORY'].value_counts()\n",
    "        for category, count in los_dist.items():\n",
    "            pct = count / len(df_clinical) * 100\n",
    "            print(f\"    {category}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # 2. DRG (Diagnosis Related Group) features\n",
    "    if 'CLM_DRG_CD' in df_clinical.columns:\n",
    "        print(\"\\nCreating DRG features...\")\n",
    "        \n",
    "        # DRG frequency encoding\n",
    "        drg_counts = df_clinical['CLM_DRG_CD'].value_counts()\n",
    "        df_clinical['DRG_FREQUENCY'] = df_clinical['CLM_DRG_CD'].map(drg_counts)\n",
    "        \n",
    "        # Common vs rare DRGs\n",
    "        common_drgs = drg_counts[drg_counts >= 100].index  # DRGs with 100+ cases\n",
    "        df_clinical['COMMON_DRG'] = df_clinical['CLM_DRG_CD'].isin(common_drgs).astype(int)\n",
    "        \n",
    "        # High-risk DRGs (top readmission rates by DRG)\n",
    "        drg_readmission_rates = df_clinical.groupby('CLM_DRG_CD')['READMISSION_30DAY'].mean()\n",
    "        high_risk_drgs = drg_readmission_rates[drg_readmission_rates > drg_readmission_rates.quantile(0.75)].index\n",
    "        df_clinical['HIGH_RISK_DRG'] = df_clinical['CLM_DRG_CD'].isin(high_risk_drgs).astype(int)\n",
    "        \n",
    "        print(f\"  ✓ DRG features created\")\n",
    "        print(f\"    Unique DRGs: {df_clinical['CLM_DRG_CD'].nunique()}\")\n",
    "        print(f\"    Common DRGs (≥100 cases): {len(common_drgs)}\")\n",
    "        print(f\"    High-risk DRGs: {len(high_risk_drgs)}\")\n",
    "    \n",
    "    # 3. Primary diagnosis features\n",
    "    if 'ICD9_DGNS_CD_1' in df_clinical.columns:\n",
    "        print(\"\\nCreating primary diagnosis features...\")\n",
    "        \n",
    "        # Diagnosis frequency\n",
    "        dx_counts = df_clinical['ICD9_DGNS_CD_1'].value_counts()\n",
    "        df_clinical['PRIMARY_DX_FREQUENCY'] = df_clinical['ICD9_DGNS_CD_1'].map(dx_counts)\n",
    "        \n",
    "        # Major diagnostic categories (first 3 digits of ICD-9)\n",
    "        df_clinical['ICD9_3DIGIT'] = df_clinical['ICD9_DGNS_CD_1'].astype(str).str[:3]\n",
    "        \n",
    "        # High-risk diagnosis groups\n",
    "        # Common high-readmission diagnoses\n",
    "        high_readmission_dx = ['428', '250', '584', '038', '486', '507', '599', '996']  # CHF, DM, AKI, sepsis, pneumonia, etc.\n",
    "        df_clinical['HIGH_READMISSION_DX'] = df_clinical['ICD9_3DIGIT'].isin(high_readmission_dx).astype(int)\n",
    "        \n",
    "        print(f\"  ✓ Primary diagnosis features created\")\n",
    "        print(f\"    Unique primary diagnoses: {df_clinical['ICD9_DGNS_CD_1'].nunique()}\")\n",
    "        print(f\"    High-readmission diagnoses: {df_clinical['HIGH_READMISSION_DX'].sum():,}\")\n",
    "    \n",
    "    # 4. Admission timing features (already created in target creation, enhance here)\n",
    "    print(\"\\nEnhancing admission timing features...\")\n",
    "    \n",
    "    # Weekend vs weekday admission risk\n",
    "    if 'WEEKEND_ADMISSION' in df_clinical.columns:\n",
    "        weekend_readmission_rate = df_clinical[df_clinical['WEEKEND_ADMISSION'] == True]['READMISSION_30DAY'].mean()\n",
    "        weekday_readmission_rate = df_clinical[df_clinical['WEEKEND_ADMISSION'] == False]['READMISSION_30DAY'].mean()\n",
    "        \n",
    "        print(f\"  Weekend readmission rate: {weekend_readmission_rate*100:.1f}%\")\n",
    "        print(f\"  Weekday readmission rate: {weekday_readmission_rate*100:.1f}%\")\n",
    "    \n",
    "    # Holiday admissions (approximation)\n",
    "    if 'ADMISSION_MONTH' in df_clinical.columns and 'ADMISSION_DAY_OF_WEEK' in df_clinical.columns:\n",
    "        # Major holidays approximation (December, January for winter holidays)\n",
    "        df_clinical['HOLIDAY_PERIOD'] = df_clinical['ADMISSION_MONTH'].isin([12, 1]).astype(int)\n",
    "        \n",
    "        # End of year admissions\n",
    "        df_clinical['END_OF_YEAR'] = (df_clinical['ADMISSION_MONTH'] == 12).astype(int)\n",
    "    \n",
    "    print(f\"\\n✓ Advanced clinical features created\")\n",
    "    \n",
    "    return df_clinical\n",
    "\n",
    "# Create clinical features\n",
    "df_with_clinical = create_clinical_features(df_with_demographics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe385fb",
   "metadata": {},
   "source": [
    "### Block 6: Chronic Condition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0b71acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING CHRONIC CONDITION FEATURES\n",
      "========================================\n",
      "Processing 11 chronic conditions...\n",
      "\n",
      "Analyzing individual conditions:\n",
      "  SP_ALZHDMTA: 12.7% readmission rate (RR: 1.25)\n",
      "  SP_CHF: 12.1% readmission rate (RR: 1.19)\n",
      "  SP_CHRNKIDN: 13.1% readmission rate (RR: 1.29)\n",
      "  SP_CNCR: 13.8% readmission rate (RR: 1.36)\n",
      "  SP_COPD: 13.8% readmission rate (RR: 1.36)\n",
      "  SP_DEPRESSN: 12.4% readmission rate (RR: 1.22)\n",
      "  SP_DIABETES: 11.5% readmission rate (RR: 1.13)\n",
      "  SP_ISCHMCHT: 11.2% readmission rate (RR: 1.10)\n",
      "  SP_OSTEOPRS: 12.0% readmission rate (RR: 1.19)\n",
      "  SP_RA_OA: 12.3% readmission rate (RR: 1.22)\n",
      "  SP_STRKETIA: 15.2% readmission rate (RR: 1.50)\n",
      "\n",
      "Identifying high-impact conditions...\n",
      "High-impact conditions: ['SP_ALZHDMTA', 'SP_CHF', 'SP_CHRNKIDN', 'SP_CNCR', 'SP_COPD', 'SP_DEPRESSN', 'SP_DIABETES', 'SP_ISCHMCHT', 'SP_OSTEOPRS', 'SP_RA_OA', 'SP_STRKETIA']\n",
      "\n",
      "Creating condition severity categories...\n",
      "\n",
      "Creating comorbidity burden indicators...\n",
      "Comorbidity distribution:\n",
      "  0 conditions: 889 patients (1.3%)\n",
      "  1 conditions: 2,293 patients (3.4%)\n",
      "  2 conditions: 4,381 patients (6.6%)\n",
      "  3 conditions: 6,430 patients (9.6%)\n",
      "  4 conditions: 8,376 patients (12.5%)\n",
      "  5 conditions: 9,996 patients (15.0%)\n",
      "  6 conditions: 10,843 patients (16.2%)\n",
      "  7 conditions: 10,164 patients (15.2%)\n",
      "\n",
      "Identifying high-risk condition combinations...\n",
      "  Diabetes + CHF: 40,459 patients (13.0% readmission rate)\n",
      "  CKD + CHF: 33,049 patients (14.4% readmission rate)\n",
      "\n",
      "✓ Chronic condition features created\n"
     ]
    }
   ],
   "source": [
    "def create_chronic_condition_features(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive chronic condition features\n",
    "    \"\"\"\n",
    "    print(\"CREATING CHRONIC CONDITION FEATURES\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    df_chronic = df.copy()\n",
    "    \n",
    "    # Get chronic condition columns\n",
    "    chronic_conditions = [col for col in df_chronic.columns if col.startswith('SP_') and col != 'SP_STATE_CODE']\n",
    "    \n",
    "    if not chronic_conditions:\n",
    "        print(\"No chronic condition columns found!\")\n",
    "        return df_chronic\n",
    "    \n",
    "    print(f\"Processing {len(chronic_conditions)} chronic conditions...\")\n",
    "    \n",
    "    # 1. Individual condition analysis\n",
    "    print(\"\\nAnalyzing individual conditions:\")\n",
    "    condition_readmission_impact = {}\n",
    "    \n",
    "    for condition in chronic_conditions:\n",
    "        if condition in df_chronic.columns:\n",
    "            # Calculate readmission rate for each condition\n",
    "            condition_rate = df_chronic[df_chronic[condition] == True]['READMISSION_30DAY'].mean()\n",
    "            overall_rate = df_chronic['READMISSION_30DAY'].mean()\n",
    "            relative_risk = condition_rate / overall_rate if overall_rate > 0 else 1.0\n",
    "            \n",
    "            condition_readmission_impact[condition] = {\n",
    "                'rate': condition_rate,\n",
    "                'relative_risk': relative_risk,\n",
    "                'count': df_chronic[condition].sum()\n",
    "            }\n",
    "            \n",
    "            print(f\"  {condition}: {condition_rate*100:.1f}% readmission rate (RR: {relative_risk:.2f})\")\n",
    "    \n",
    "    # 2. High-impact chronic conditions\n",
    "    print(\"\\nIdentifying high-impact conditions...\")\n",
    "    \n",
    "    # Conditions with relative risk > 1.1 (10% higher than average)\n",
    "    high_impact_conditions = [\n",
    "        condition for condition, stats in condition_readmission_impact.items()\n",
    "        if stats['relative_risk'] > 1.1 and stats['count'] >= 100  # At least 100 patients\n",
    "    ]\n",
    "    \n",
    "    print(f\"High-impact conditions: {high_impact_conditions}\")\n",
    "    \n",
    "    # Create high-impact condition count\n",
    "    if high_impact_conditions:\n",
    "        df_chronic['HIGH_IMPACT_CONDITIONS'] = df_chronic[high_impact_conditions].sum(axis=1)\n",
    "    else:\n",
    "        df_chronic['HIGH_IMPACT_CONDITIONS'] = 0\n",
    "    \n",
    "    # 3. Condition severity categories\n",
    "    print(\"\\nCreating condition severity categories...\")\n",
    "    \n",
    "    # Cardiovascular conditions\n",
    "    cardio_conditions = ['SP_CHF', 'SP_ISCHMCHT']\n",
    "    available_cardio = [col for col in cardio_conditions if col in df_chronic.columns]\n",
    "    if available_cardio:\n",
    "        df_chronic['CARDIOVASCULAR_CONDITIONS'] = df_chronic[available_cardio].sum(axis=1)\n",
    "        df_chronic['HAS_CARDIOVASCULAR'] = (df_chronic['CARDIOVASCULAR_CONDITIONS'] > 0).astype(int)\n",
    "    \n",
    "    # Metabolic conditions\n",
    "    metabolic_conditions = ['SP_DIABETES', 'SP_CHRNKIDN']\n",
    "    available_metabolic = [col for col in metabolic_conditions if col in df_chronic.columns]\n",
    "    if available_metabolic:\n",
    "        df_chronic['METABOLIC_CONDITIONS'] = df_chronic[available_metabolic].sum(axis=1)\n",
    "        df_chronic['HAS_METABOLIC'] = (df_chronic['METABOLIC_CONDITIONS'] > 0).astype(int)\n",
    "    \n",
    "    # Mental health conditions\n",
    "    mental_conditions = ['SP_DEPRESSN', 'SP_ALZHDMTA']\n",
    "    available_mental = [col for col in mental_conditions if col in df_chronic.columns]\n",
    "    if available_mental:\n",
    "        df_chronic['MENTAL_HEALTH_CONDITIONS'] = df_chronic[available_mental].sum(axis=1)\n",
    "        df_chronic['HAS_MENTAL_HEALTH'] = (df_chronic['MENTAL_HEALTH_CONDITIONS'] > 0).astype(int)\n",
    "    \n",
    "    # Respiratory conditions\n",
    "    respiratory_conditions = ['SP_COPD']\n",
    "    available_respiratory = [col for col in respiratory_conditions if col in df_chronic.columns]\n",
    "    if available_respiratory:\n",
    "        df_chronic['RESPIRATORY_CONDITIONS'] = df_chronic[available_respiratory].sum(axis=1)\n",
    "        df_chronic['HAS_RESPIRATORY'] = (df_chronic['RESPIRATORY_CONDITIONS'] > 0).astype(int)\n",
    "    \n",
    "    # 4. Comorbidity burden indicators\n",
    "    print(\"\\nCreating comorbidity burden indicators...\")\n",
    "    \n",
    "    # Enhanced chronic condition count (already exists, but validate)\n",
    "    if 'CHRONIC_CONDITION_COUNT' in df_chronic.columns:\n",
    "        # Comorbidity categories\n",
    "        df_chronic['NO_COMORBIDITIES'] = (df_chronic['CHRONIC_CONDITION_COUNT'] == 0).astype(int)\n",
    "        df_chronic['LOW_COMORBIDITY'] = (df_chronic['CHRONIC_CONDITION_COUNT'].between(1, 2)).astype(int)\n",
    "        df_chronic['MODERATE_COMORBIDITY'] = (df_chronic['CHRONIC_CONDITION_COUNT'].between(3, 5)).astype(int)\n",
    "        df_chronic['HIGH_COMORBIDITY'] = (df_chronic['CHRONIC_CONDITION_COUNT'] >= 6).astype(int)\n",
    "        \n",
    "        # Complex patient indicator (multiple body systems affected)\n",
    "        system_conditions = []\n",
    "        if available_cardio:\n",
    "            system_conditions.append('HAS_CARDIOVASCULAR')\n",
    "        if available_metabolic:\n",
    "            system_conditions.append('HAS_METABOLIC')\n",
    "        if available_mental:\n",
    "            system_conditions.append('HAS_MENTAL_HEALTH')\n",
    "        if available_respiratory:\n",
    "            system_conditions.append('HAS_RESPIRATORY')\n",
    "        \n",
    "        if len(system_conditions) >= 2:\n",
    "            df_chronic['MULTI_SYSTEM_DISEASE'] = (df_chronic[system_conditions].sum(axis=1) >= 2).astype(int)\n",
    "        \n",
    "        # Print comorbidity distribution\n",
    "        print(f\"Comorbidity distribution:\")\n",
    "        comorbidity_dist = df_chronic['CHRONIC_CONDITION_COUNT'].value_counts().sort_index()\n",
    "        for count, patients in comorbidity_dist.head(8).items():\n",
    "            pct = patients / len(df_chronic) * 100\n",
    "            print(f\"  {count} conditions: {patients:,} patients ({pct:.1f}%)\")\n",
    "    \n",
    "    # 5. Specific high-risk combinations\n",
    "    print(\"\\nIdentifying high-risk condition combinations...\")\n",
    "    \n",
    "    # Diabetes + CHF (common high-risk combination)\n",
    "    if 'SP_DIABETES' in df_chronic.columns and 'SP_CHF' in df_chronic.columns:\n",
    "        df_chronic['DIABETES_CHF_COMBO'] = (\n",
    "            (df_chronic['SP_DIABETES'] == True) & (df_chronic['SP_CHF'] == True)\n",
    "        ).astype(int)\n",
    "        \n",
    "        combo_count = df_chronic['DIABETES_CHF_COMBO'].sum()\n",
    "        if combo_count > 0:\n",
    "            combo_rate = df_chronic[df_chronic['DIABETES_CHF_COMBO'] == 1]['READMISSION_30DAY'].mean()\n",
    "            print(f\"  Diabetes + CHF: {combo_count:,} patients ({combo_rate*100:.1f}% readmission rate)\")\n",
    "    \n",
    "    # CKD + CHF (kidney-heart syndrome)\n",
    "    if 'SP_CHRNKIDN' in df_chronic.columns and 'SP_CHF' in df_chronic.columns:\n",
    "        df_chronic['CKD_CHF_COMBO'] = (\n",
    "            (df_chronic['SP_CHRNKIDN'] == True) & (df_chronic['SP_CHF'] == True)\n",
    "        ).astype(int)\n",
    "        \n",
    "        combo_count = df_chronic['CKD_CHF_COMBO'].sum()\n",
    "        if combo_count > 0:\n",
    "            combo_rate = df_chronic[df_chronic['CKD_CHF_COMBO'] == 1]['READMISSION_30DAY'].mean()\n",
    "            print(f\"  CKD + CHF: {combo_count:,} patients ({combo_rate*100:.1f}% readmission rate)\")\n",
    "    \n",
    "    print(f\"\\n✓ Chronic condition features created\")\n",
    "    \n",
    "    return df_chronic\n",
    "\n",
    "# Create chronic condition features\n",
    "df_with_chronic = create_chronic_condition_features(df_with_clinical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8c883",
   "metadata": {},
   "source": [
    "### Block 7: Prior Admission History Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ffe3a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING PRIOR ADMISSION HISTORY FEATURES\n",
      "=============================================\n",
      "Analyzing prior admissions for 66,773 index admissions...\n",
      "  Processing batch 1: records 0 to 5,000\n",
      "  Processing batch 2: records 5,000 to 10,000\n",
      "    Processed 10,000 records...\n",
      "  Processing batch 3: records 10,000 to 15,000\n",
      "  Processing batch 4: records 15,000 to 20,000\n",
      "    Processed 20,000 records...\n",
      "  Processing batch 5: records 20,000 to 25,000\n",
      "  Processing batch 6: records 25,000 to 30,000\n",
      "    Processed 30,000 records...\n",
      "  Processing batch 7: records 30,000 to 35,000\n",
      "  Processing batch 8: records 35,000 to 40,000\n",
      "    Processed 40,000 records...\n",
      "  Processing batch 9: records 40,000 to 45,000\n",
      "  Processing batch 10: records 45,000 to 50,000\n",
      "    Processed 50,000 records...\n",
      "  Processing batch 11: records 50,000 to 55,000\n",
      "  Processing batch 12: records 55,000 to 60,000\n",
      "    Processed 60,000 records...\n",
      "  Processing batch 13: records 60,000 to 65,000\n",
      "  Processing batch 14: records 65,000 to 66,773\n",
      "\n",
      "Creating derived prior admission features...\n",
      "\n",
      "PRIOR ADMISSION STATISTICS:\n",
      "├── Patients with prior admissions (365d): 25,011\n",
      "├── Frequent flyers (≥3 in 365d): 4,270\n",
      "├── High utilizers (≥2 in 90d): 3,019\n",
      "├── Recent admissions (30d): 6,756\n",
      "└── Mean prior admissions (365d): 0.6\n",
      "\n",
      "READMISSION RATES BY PRIOR HISTORY:\n",
      "├── Non-frequent flyers: 9.8%\n",
      "├── Frequent flyers: 15.5%\n",
      "├── No recent admission: 9.7%\n",
      "└── Recent admission (30d): 14.4%\n",
      "\n",
      "✓ Prior admission history features created\n"
     ]
    }
   ],
   "source": [
    "def create_prior_admission_features(df_target, df_inpatient_all):\n",
    "    \"\"\"\n",
    "    Create features based on prior admission history\n",
    "    \"\"\"\n",
    "    print(\"CREATING PRIOR ADMISSION HISTORY FEATURES\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    df_history = df_target.copy()\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    if 'CLM_ADMSN_DT_DATE' not in df_inpatient_all.columns:\n",
    "        df_inpatient_all['CLM_ADMSN_DT_DATE'] = pd.to_datetime(\n",
    "            df_inpatient_all['CLM_ADMSN_DT'].astype(str), \n",
    "            format='%Y%m%d', \n",
    "            errors='coerce'\n",
    "        )\n",
    "    \n",
    "    if 'NCH_BENE_DSCHRG_DT_DATE' not in df_inpatient_all.columns:\n",
    "        df_inpatient_all['NCH_BENE_DSCHRG_DT_DATE'] = pd.to_datetime(\n",
    "            df_inpatient_all['NCH_BENE_DSCHRG_DT'].astype(str), \n",
    "            format='%Y%m%d', \n",
    "            errors='coerce'\n",
    "        )\n",
    "    \n",
    "    # Add LOS to inpatient data if not present\n",
    "    if 'LOS_CALCULATED' not in df_inpatient_all.columns:\n",
    "        df_inpatient_all['LOS_CALCULATED'] = (\n",
    "            df_inpatient_all['NCH_BENE_DSCHRG_DT_DATE'] - df_inpatient_all['CLM_ADMSN_DT_DATE']\n",
    "        ).dt.days + 1\n",
    "    \n",
    "    print(f\"Analyzing prior admissions for {len(df_history):,} index admissions...\")\n",
    "    \n",
    "    # Initialize prior admission features\n",
    "    df_history['PRIOR_ADMISSIONS_30D'] = 0\n",
    "    df_history['PRIOR_ADMISSIONS_90D'] = 0\n",
    "    df_history['PRIOR_ADMISSIONS_180D'] = 0\n",
    "    df_history['PRIOR_ADMISSIONS_365D'] = 0\n",
    "    df_history['DAYS_SINCE_LAST_ADMISSION'] = np.nan\n",
    "    df_history['PRIOR_TOTAL_LOS_365D'] = 0\n",
    "    df_history['PRIOR_AVG_LOS_365D'] = np.nan\n",
    "    df_history['FREQUENT_FLYER'] = 0\n",
    "    df_history['PRIOR_ICU_ADMISSIONS_365D'] = 0  # Proxy using very long stays\n",
    "    \n",
    "    # Process in batches for memory efficiency\n",
    "    batch_size = 5000\n",
    "    processed = 0\n",
    "    \n",
    "    for i in range(0, len(df_history), batch_size):\n",
    "        batch_end = min(i + batch_size, len(df_history))\n",
    "        batch = df_history.iloc[i:batch_end].copy()\n",
    "        \n",
    "        print(f\"  Processing batch {i//batch_size + 1}: records {i:,} to {batch_end:,}\")\n",
    "        \n",
    "        for idx, admission in batch.iterrows():\n",
    "            beneficiary_id = admission['DESYNPUF_ID']\n",
    "            index_admission_date = admission['CLM_ADMSN_DT_DATE']\n",
    "            \n",
    "            # Find all prior admissions for this beneficiary\n",
    "            prior_admissions = df_inpatient_all[\n",
    "                (df_inpatient_all['DESYNPUF_ID'] == beneficiary_id) &\n",
    "                (df_inpatient_all['NCH_BENE_DSCHRG_DT_DATE'] < index_admission_date) &\n",
    "                (df_inpatient_all['CLM_ADMSN_DT_DATE'].notna())\n",
    "            ].copy()\n",
    "            \n",
    "            if len(prior_admissions) > 0:\n",
    "                # Calculate days between discharge and current admission\n",
    "                prior_admissions['DAYS_BETWEEN'] = (\n",
    "                    index_admission_date - prior_admissions['NCH_BENE_DSCHRG_DT_DATE']\n",
    "                ).dt.days\n",
    "                \n",
    "                # Prior admissions in different time windows\n",
    "                df_history.loc[idx, 'PRIOR_ADMISSIONS_30D'] = len(\n",
    "                    prior_admissions[prior_admissions['DAYS_BETWEEN'] <= 30]\n",
    "                )\n",
    "                df_history.loc[idx, 'PRIOR_ADMISSIONS_90D'] = len(\n",
    "                    prior_admissions[prior_admissions['DAYS_BETWEEN'] <= 90]\n",
    "                )\n",
    "                df_history.loc[idx, 'PRIOR_ADMISSIONS_180D'] = len(\n",
    "                    prior_admissions[prior_admissions['DAYS_BETWEEN'] <= 180]\n",
    "                )\n",
    "                df_history.loc[idx, 'PRIOR_ADMISSIONS_365D'] = len(\n",
    "                    prior_admissions[prior_admissions['DAYS_BETWEEN'] <= 365]\n",
    "                )\n",
    "                \n",
    "                # Days since last admission\n",
    "                df_history.loc[idx, 'DAYS_SINCE_LAST_ADMISSION'] = prior_admissions['DAYS_BETWEEN'].min()\n",
    "                \n",
    "                # Prior length of stay metrics (last 365 days)\n",
    "                prior_365d = prior_admissions[prior_admissions['DAYS_BETWEEN'] <= 365]\n",
    "                if len(prior_365d) > 0:\n",
    "                    df_history.loc[idx, 'PRIOR_TOTAL_LOS_365D'] = prior_365d['LOS_CALCULATED'].sum()\n",
    "                    df_history.loc[idx, 'PRIOR_AVG_LOS_365D'] = prior_365d['LOS_CALCULATED'].mean()\n",
    "                    \n",
    "                    # Proxy for ICU admissions (very long stays ≥ 14 days)\n",
    "                    df_history.loc[idx, 'PRIOR_ICU_ADMISSIONS_365D'] = len(\n",
    "                        prior_365d[prior_365d['LOS_CALCULATED'] >= 14]\n",
    "                    )\n",
    "                \n",
    "                # Frequent flyer indicator (≥3 admissions in past 365 days)\n",
    "                if df_history.loc[idx, 'PRIOR_ADMISSIONS_365D'] >= 3:\n",
    "                    df_history.loc[idx, 'FREQUENT_FLYER'] = 1\n",
    "        \n",
    "        processed += len(batch)\n",
    "        if processed % 10000 == 0:\n",
    "           print(f\"    Processed {processed:,} records...\")\n",
    "   \n",
    "    # Create additional derived features\n",
    "    print(\"\\nCreating derived prior admission features...\")\n",
    "   \n",
    "    # Recent admission indicators\n",
    "    df_history['RECENT_ADMISSION_30D'] = (df_history['PRIOR_ADMISSIONS_30D'] > 0).astype(int)\n",
    "    df_history['RECENT_ADMISSION_90D'] = (df_history['PRIOR_ADMISSIONS_90D'] > 0).astype(int)\n",
    "   \n",
    "    # High utilization indicators\n",
    "    df_history['HIGH_UTILIZER_90D'] = (df_history['PRIOR_ADMISSIONS_90D'] >= 2).astype(int)\n",
    "    df_history['HIGH_UTILIZER_365D'] = (df_history['PRIOR_ADMISSIONS_365D'] >= 3).astype(int)\n",
    "   \n",
    "    # Days since last admission categories\n",
    "    df_history['DAYS_SINCE_LAST_CAT'] = pd.cut(\n",
    "       df_history['DAYS_SINCE_LAST_ADMISSION'],\n",
    "       bins=[-1, 7, 30, 90, 180, 365, 9999],\n",
    "       labels=['No_Prior', 'Within_7d', 'Within_30d', 'Within_90d', 'Within_180d', 'Within_365d'],\n",
    "       right=True\n",
    "    )\n",
    "   \n",
    "    # Average LOS categories\n",
    "    df_history['PRIOR_AVG_LOS_CAT'] = pd.cut(\n",
    "       df_history['PRIOR_AVG_LOS_365D'],\n",
    "       bins=[0, 3, 7, 14, 999],\n",
    "       labels=['Short_Avg', 'Medium_Avg', 'Long_Avg', 'Very_Long_Avg'],\n",
    "       right=False\n",
    "    )\n",
    "   \n",
    "    # Summary statistics\n",
    "    print(f\"\\nPRIOR ADMISSION STATISTICS:\")\n",
    "    print(f\"├── Patients with prior admissions (365d): {(df_history['PRIOR_ADMISSIONS_365D'] > 0).sum():,}\")\n",
    "    print(f\"├── Frequent flyers (≥3 in 365d): {df_history['FREQUENT_FLYER'].sum():,}\")\n",
    "    print(f\"├── High utilizers (≥2 in 90d): {df_history['HIGH_UTILIZER_90D'].sum():,}\")\n",
    "    print(f\"├── Recent admissions (30d): {df_history['RECENT_ADMISSION_30D'].sum():,}\")\n",
    "    print(f\"└── Mean prior admissions (365d): {df_history['PRIOR_ADMISSIONS_365D'].mean():.1f}\")\n",
    "   \n",
    "    # Readmission rate by prior admission history\n",
    "    print(f\"\\nREADMISSION RATES BY PRIOR HISTORY:\")\n",
    "   \n",
    "    # By frequent flyer status\n",
    "    ff_rates = df_history.groupby('FREQUENT_FLYER')['READMISSION_30DAY'].mean()\n",
    "    print(f\"├── Non-frequent flyers: {ff_rates[0]*100:.1f}%\")\n",
    "    print(f\"├── Frequent flyers: {ff_rates[1]*100:.1f}%\")\n",
    "   \n",
    "    # By recent admission status\n",
    "    recent_rates = df_history.groupby('RECENT_ADMISSION_30D')['READMISSION_30DAY'].mean()\n",
    "    print(f\"├── No recent admission: {recent_rates[0]*100:.1f}%\")\n",
    "    print(f\"└── Recent admission (30d): {recent_rates[1]*100:.1f}%\")\n",
    "   \n",
    "    print(f\"\\n✓ Prior admission history features created\")\n",
    "   \n",
    "    return df_history\n",
    "\n",
    "# Create prior admission features\n",
    "df_with_history = create_prior_admission_features(df_with_chronic, df_inpatient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc9aa9",
   "metadata": {},
   "source": [
    "### Block 8: Advanced Risk Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9fab443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING ADVANCED RISK INDICATORS\n",
      "========================================\n",
      "Creating composite risk scores...\n",
      "Creating comprehensive risk score...\n",
      "Risk category distribution:\n",
      "  Moderate_Risk: 34,150 (51.1%) - 8.2% readmission rate\n",
      "  High_Risk: 25,552 (38.3%) - 12.8% readmission rate\n",
      "  Low_Risk: 4,129 (6.2%) - 4.3% readmission rate\n",
      "  Very_High_Risk: 2,942 (4.4%) - 17.2% readmission rate\n",
      "\n",
      "Identifying high-risk patient profiles...\n",
      "  Elderly + Complex: 12,192 (14.6% readmission rate)\n",
      "  Frequent + Recent: 1,454 (17.1% readmission rate)\n",
      "  Complex Cardiac: 34,304 (14.6% readmission rate)\n",
      "\n",
      "Creating discharge timing risk factors...\n",
      "  Weekend discharges: 18,836 (10.3% readmission rate)\n",
      "  Premature discharge risk: 7,418 (9.9% readmission rate)\n",
      "\n",
      "Creating social determinants proxies...\n",
      "\n",
      "✓ Advanced risk indicators created\n"
     ]
    }
   ],
   "source": [
    "def create_advanced_risk_indicators(df):\n",
    "    \"\"\"\n",
    "    Create advanced risk indicators combining multiple feature types\n",
    "    \"\"\"\n",
    "    print(\"CREATING ADVANCED RISK INDICATORS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    df_risk = df.copy()\n",
    "    \n",
    "    # 1. Composite risk scores\n",
    "    print(\"Creating composite risk scores...\")\n",
    "    \n",
    "    # Age-based risk (higher weight for older patients)\n",
    "    if 'AGE_AT_ADMISSION' in df_risk.columns:\n",
    "        df_risk['AGE_RISK_SCORE'] = np.where(\n",
    "            df_risk['AGE_AT_ADMISSION'] >= 85, 3,\n",
    "            np.where(df_risk['AGE_AT_ADMISSION'] >= 75, 2,\n",
    "                    np.where(df_risk['AGE_AT_ADMISSION'] >= 65, 1, 0))\n",
    "        )\n",
    "    \n",
    "    # Comorbidity risk score\n",
    "    if 'CHRONIC_CONDITION_COUNT' in df_risk.columns:\n",
    "        df_risk['COMORBIDITY_RISK_SCORE'] = np.where(\n",
    "            df_risk['CHRONIC_CONDITION_COUNT'] >= 6, 3,\n",
    "            np.where(df_risk['CHRONIC_CONDITION_COUNT'] >= 3, 2,\n",
    "                    np.where(df_risk['CHRONIC_CONDITION_COUNT'] >= 1, 1, 0))\n",
    "        )\n",
    "    \n",
    "    # Prior utilization risk score\n",
    "    if 'PRIOR_ADMISSIONS_365D' in df_risk.columns:\n",
    "        df_risk['UTILIZATION_RISK_SCORE'] = np.where(\n",
    "            df_risk['PRIOR_ADMISSIONS_365D'] >= 4, 3,\n",
    "            np.where(df_risk['PRIOR_ADMISSIONS_365D'] >= 2, 2,\n",
    "                    np.where(df_risk['PRIOR_ADMISSIONS_365D'] >= 1, 1, 0))\n",
    "        )\n",
    "    \n",
    "    # Clinical complexity risk score\n",
    "    clinical_risk_factors = []\n",
    "    if 'LONG_STAY' in df_risk.columns:\n",
    "        clinical_risk_factors.append('LONG_STAY')\n",
    "    if 'HIGH_RISK_DRG' in df_risk.columns:\n",
    "        clinical_risk_factors.append('HIGH_RISK_DRG')\n",
    "    if 'HIGH_READMISSION_DX' in df_risk.columns:\n",
    "        clinical_risk_factors.append('HIGH_READMISSION_DX')\n",
    "    \n",
    "    if clinical_risk_factors:\n",
    "        df_risk['CLINICAL_COMPLEXITY_SCORE'] = df_risk[clinical_risk_factors].sum(axis=1)\n",
    "    \n",
    "    # 2. Comprehensive risk score\n",
    "    print(\"Creating comprehensive risk score...\")\n",
    "    \n",
    "    risk_components = []\n",
    "    if 'AGE_RISK_SCORE' in df_risk.columns:\n",
    "        risk_components.append('AGE_RISK_SCORE')\n",
    "    if 'COMORBIDITY_RISK_SCORE' in df_risk.columns:\n",
    "        risk_components.append('COMORBIDITY_RISK_SCORE')\n",
    "    if 'UTILIZATION_RISK_SCORE' in df_risk.columns:\n",
    "        risk_components.append('UTILIZATION_RISK_SCORE')\n",
    "    if 'CLINICAL_COMPLEXITY_SCORE' in df_risk.columns:\n",
    "        risk_components.append('CLINICAL_COMPLEXITY_SCORE')\n",
    "    \n",
    "    if risk_components:\n",
    "        df_risk['COMPREHENSIVE_RISK_SCORE'] = df_risk[risk_components].sum(axis=1)\n",
    "        \n",
    "        # Risk categories\n",
    "        df_risk['RISK_CATEGORY'] = pd.cut(\n",
    "            df_risk['COMPREHENSIVE_RISK_SCORE'],\n",
    "            bins=[-1, 2, 5, 8, 20],\n",
    "            labels=['Low_Risk', 'Moderate_Risk', 'High_Risk', 'Very_High_Risk'],\n",
    "            right=True\n",
    "        )\n",
    "        \n",
    "        # Print risk distribution\n",
    "        risk_dist = df_risk['RISK_CATEGORY'].value_counts()\n",
    "        print(f\"Risk category distribution:\")\n",
    "        for category, count in risk_dist.items():\n",
    "            pct = count / len(df_risk) * 100\n",
    "            readmission_rate = df_risk[df_risk['RISK_CATEGORY'] == category]['READMISSION_30DAY'].mean()\n",
    "            print(f\"  {category}: {count:,} ({pct:.1f}%) - {readmission_rate*100:.1f}% readmission rate\")\n",
    "    \n",
    "    # 3. Specific high-risk patient profiles\n",
    "    print(\"\\nIdentifying high-risk patient profiles...\")\n",
    "    \n",
    "    # Elderly with multiple comorbidities\n",
    "    if 'HIGH_RISK_AGE' in df_risk.columns and 'HIGH_COMORBIDITY' in df_risk.columns:\n",
    "        df_risk['ELDERLY_COMPLEX'] = (\n",
    "            (df_risk['HIGH_RISK_AGE'] == 1) & (df_risk['HIGH_COMORBIDITY'] == 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        elderly_complex_count = df_risk['ELDERLY_COMPLEX'].sum()\n",
    "        if elderly_complex_count > 0:\n",
    "            elderly_complex_rate = df_risk[df_risk['ELDERLY_COMPLEX'] == 1]['READMISSION_30DAY'].mean()\n",
    "            print(f\"  Elderly + Complex: {elderly_complex_count:,} ({elderly_complex_rate*100:.1f}% readmission rate)\")\n",
    "    \n",
    "    # Frequent flyer with recent admission\n",
    "    if 'FREQUENT_FLYER' in df_risk.columns and 'RECENT_ADMISSION_30D' in df_risk.columns:\n",
    "        df_risk['FREQUENT_RECENT'] = (\n",
    "            (df_risk['FREQUENT_FLYER'] == 1) & (df_risk['RECENT_ADMISSION_30D'] == 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        freq_recent_count = df_risk['FREQUENT_RECENT'].sum()\n",
    "        if freq_recent_count > 0:\n",
    "            freq_recent_rate = df_risk[df_risk['FREQUENT_RECENT'] == 1]['READMISSION_30DAY'].mean()\n",
    "            print(f\"  Frequent + Recent: {freq_recent_count:,} ({freq_recent_rate*100:.1f}% readmission rate)\")\n",
    "    \n",
    "    # Complex cardiac patients\n",
    "    if 'HAS_CARDIOVASCULAR' in df_risk.columns and 'HIGH_COMORBIDITY' in df_risk.columns:\n",
    "        df_risk['COMPLEX_CARDIAC'] = (\n",
    "            (df_risk['HAS_CARDIOVASCULAR'] == 1) & (df_risk['HIGH_COMORBIDITY'] == 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        complex_cardiac_count = df_risk['COMPLEX_CARDIAC'].sum()\n",
    "        if complex_cardiac_count > 0:\n",
    "            complex_cardiac_rate = df_risk[df_risk['COMPLEX_CARDIAC'] == 1]['READMISSION_30DAY'].mean()\n",
    "            print(f\"  Complex Cardiac: {complex_cardiac_count:,} ({complex_cardiac_rate*100:.1f}% readmission rate)\")\n",
    "    \n",
    "    # 4. Discharge timing risk factors\n",
    "    print(\"\\nCreating discharge timing risk factors...\")\n",
    "    \n",
    "    # Weekend discharge\n",
    "    if 'NCH_BENE_DSCHRG_DT_DATE' in df_risk.columns:\n",
    "        df_risk['DISCHARGE_DAY_OF_WEEK'] = df_risk['NCH_BENE_DSCHRG_DT_DATE'].dt.dayofweek\n",
    "        df_risk['WEEKEND_DISCHARGE'] = df_risk['DISCHARGE_DAY_OF_WEEK'].isin([4, 5]).astype(int)  # Friday, Saturday\n",
    "        \n",
    "        weekend_discharge_count = df_risk['WEEKEND_DISCHARGE'].sum()\n",
    "        if weekend_discharge_count > 0:\n",
    "            weekend_discharge_rate = df_risk[df_risk['WEEKEND_DISCHARGE'] == 1]['READMISSION_30DAY'].mean()\n",
    "            print(f\"  Weekend discharges: {weekend_discharge_count:,} ({weekend_discharge_rate*100:.1f}% readmission rate)\")\n",
    "    \n",
    "    # Short stay + high comorbidity (potentially premature discharge)\n",
    "    if 'SHORT_STAY' in df_risk.columns and 'MODERATE_COMORBIDITY' in df_risk.columns:\n",
    "        df_risk['PREMATURE_DISCHARGE_RISK'] = (\n",
    "            (df_risk['SHORT_STAY'] == 1) & \n",
    "            ((df_risk['MODERATE_COMORBIDITY'] == 1) | (df_risk['HIGH_COMORBIDITY'] == 1))\n",
    "        ).astype(int)\n",
    "        \n",
    "        premature_count = df_risk['PREMATURE_DISCHARGE_RISK'].sum()\n",
    "        if premature_count > 0:\n",
    "            premature_rate = df_risk[df_risk['PREMATURE_DISCHARGE_RISK'] == 1]['READMISSION_30DAY'].mean()\n",
    "            print(f\"  Premature discharge risk: {premature_count:,} ({premature_rate*100:.1f}% readmission rate)\")\n",
    "    \n",
    "    # 5. Social determinants proxies\n",
    "    print(\"\\nCreating social determinants proxies...\")\n",
    "    \n",
    "    # High-volume state as proxy for healthcare access\n",
    "    if 'HIGH_VOLUME_STATE' in df_risk.columns:\n",
    "        low_access_proxy = 1 - df_risk['HIGH_VOLUME_STATE']  # Inverse of high-volume state\n",
    "        df_risk['LIMITED_ACCESS_PROXY'] = low_access_proxy\n",
    "    \n",
    "    # Minority status + high comorbidity (health disparities)\n",
    "    if 'IS_MINORITY' in df_risk.columns and 'HIGH_COMORBIDITY' in df_risk.columns:\n",
    "        df_risk['DISPARITY_RISK'] = (\n",
    "            (df_risk['IS_MINORITY'] == 1) & (df_risk['HIGH_COMORBIDITY'] == 1)\n",
    "        ).astype(int)\n",
    "    \n",
    "    print(f\"\\n✓ Advanced risk indicators created\")\n",
    "    \n",
    "    return df_risk\n",
    "\n",
    "# Create advanced risk indicators\n",
    "df_with_risk = create_advanced_risk_indicators(df_with_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88bec9b",
   "metadata": {},
   "source": [
    "### Block 9: Feature Selection and Engineering Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94026a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE VALIDATION AND SELECTION\n",
      "========================================\n",
      "Assessing feature completeness...\n",
      "├── Complete features (100%): 84\n",
      "├── Mostly complete (95-99%): 27\n",
      "└── Incomplete features (<95%): 6\n",
      "\n",
      "Incomplete features:\n",
      "  DAYS_TO_READMISSION: 10.1% complete\n",
      "  READMISSION_DATE: 10.1% complete\n",
      "  DAYS_SINCE_LAST_ADMISSION: 41.6% complete\n",
      "  PRIOR_AVG_LOS_365D: 37.5% complete\n",
      "  DAYS_SINCE_LAST_CAT: 41.6% complete\n",
      "  PRIOR_AVG_LOS_CAT: 37.5% complete\n",
      "\n",
      "Analyzing feature correlation with target...\n",
      "Top 15 features by correlation with readmission:\n",
      "   1. CHRONIC_CONDITION_COUNT: 0.1772\n",
      "   2. COMPLEX_CARDIAC: 0.1519\n",
      "   3. HIGH_COMORBIDITY: 0.1516\n",
      "   4. NCH_BENE_DSCHRG_DT: 0.1513\n",
      "   5. CLM_ADMSN_DT: 0.1507\n",
      "   6. discharge_year: 0.1492\n",
      "   7. DISCHARGE_YEAR: 0.1492\n",
      "   8. ADMISSION_YEAR: 0.1484\n",
      "   9. BENE_YEAR: 0.1481\n",
      "  10. COMORBIDITY_RISK_SCORE: 0.1466\n",
      "  11. CKD_CHF_COMBO: 0.1407\n",
      "  12. DIABETES_CHF_COMBO: 0.1193\n",
      "  13. HAS_RESPIRATORY: 0.1185\n",
      "  14. COMPREHENSIVE_RISK_SCORE: 0.1090\n",
      "  15. MODERATE_COMORBIDITY: 0.0997\n",
      "\n",
      "Analyzing categorical features...\n",
      "Top 10 categorical features by rate variance:\n",
      "   1. ICD9_DGNS_CD_1: 0.043121\n",
      "   2. ICD9_3DIGIT: 0.028723\n",
      "   3. DESYNPUF_ID: 0.021636\n",
      "   4. HIGH_IMPACT_CONDITIONS: 0.007378\n",
      "   5. RISK_CATEGORY: 0.003114\n",
      "   6. SP_CHRNKIDN: 0.002760\n",
      "   7. SP_COPD: 0.002614\n",
      "   8. SP_CHF: 0.002556\n",
      "   9. RESPIRATORY_CONDITIONS: 0.002556\n",
      "  10. METABOLIC_CONDITIONS: 0.002537\n",
      "\n",
      "Categorizing features by type and importance...\n",
      "Feature categories:\n",
      "├── High-value features: 44\n",
      "├── Essential demographic: 4\n",
      "├── Essential clinical: 6\n",
      "├── Essential chronic conditions: 5\n",
      "├── Essential history: 5\n",
      "└── Risk indicators: 5\n",
      "\n",
      "Recommended modeling features: 55 features + target\n"
     ]
    }
   ],
   "source": [
    "def validate_and_select_features(df):\n",
    "    \"\"\"\n",
    "    Validate engineered features and select best features for modeling\n",
    "    \"\"\"\n",
    "    print(\"FEATURE VALIDATION AND SELECTION\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # 1. Feature completeness assessment\n",
    "    print(\"Assessing feature completeness...\")\n",
    "    \n",
    "    feature_completeness = {}\n",
    "    total_records = len(df)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != 'READMISSION_30DAY':  # Exclude target variable\n",
    "            missing_count = df[col].isnull().sum()\n",
    "            completeness_pct = (1 - missing_count / total_records) * 100\n",
    "            feature_completeness[col] = completeness_pct\n",
    "    \n",
    "    # Categorize features by completeness\n",
    "    complete_features = [col for col, pct in feature_completeness.items() if pct == 100]\n",
    "    mostly_complete = [col for col, pct in feature_completeness.items() if 95 <= pct < 100]\n",
    "    incomplete_features = [col for col, pct in feature_completeness.items() if pct < 95]\n",
    "    \n",
    "    print(f\"├── Complete features (100%): {len(complete_features)}\")\n",
    "    print(f\"├── Mostly complete (95-99%): {len(mostly_complete)}\")\n",
    "    print(f\"└── Incomplete features (<95%): {len(incomplete_features)}\")\n",
    "    \n",
    "    if incomplete_features:\n",
    "        print(f\"\\nIncomplete features:\")\n",
    "        for feature in incomplete_features[:10]:  # Show first 10\n",
    "            pct = feature_completeness[feature]\n",
    "            print(f\"  {feature}: {pct:.1f}% complete\")\n",
    "    \n",
    "    # 2. Feature correlation with target variable\n",
    "    print(f\"\\nAnalyzing feature correlation with target...\")\n",
    "    \n",
    "    # Calculate correlation for numeric features\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_features = [col for col in numeric_features if col != 'READMISSION_30DAY']\n",
    "    \n",
    "    feature_correlations = {}\n",
    "    for feature in numeric_features:\n",
    "        if df[feature].notna().sum() > 100:  # At least 100 non-null values\n",
    "            correlation = df[feature].corr(df['READMISSION_30DAY'])\n",
    "            if not np.isnan(correlation):\n",
    "                feature_correlations[feature] = abs(correlation)\n",
    "    \n",
    "    # Sort by correlation strength\n",
    "    sorted_correlations = sorted(feature_correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Top 15 features by correlation with readmission:\")\n",
    "    for i, (feature, corr) in enumerate(sorted_correlations[:15]):\n",
    "        print(f\"  {i+1:2d}. {feature}: {corr:.4f}\")\n",
    "    \n",
    "    # 3. Categorical feature analysis\n",
    "    print(f\"\\nAnalyzing categorical features...\")\n",
    "    \n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_readmission_rates = {}\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        if df[feature].notna().sum() > 100:\n",
    "            rates = df.groupby(feature)['READMISSION_30DAY'].agg(['count', 'mean']).round(4)\n",
    "            # Calculate variance in readmission rates across categories\n",
    "            rate_variance = rates['mean'].var()\n",
    "            categorical_readmission_rates[feature] = rate_variance\n",
    "    \n",
    "    # Sort by variance (features with more discriminative categories)\n",
    "    sorted_categorical = sorted(categorical_readmission_rates.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Top 10 categorical features by rate variance:\")\n",
    "    for i, (feature, variance) in enumerate(sorted_categorical[:10]):\n",
    "        print(f\"  {i+1:2d}. {feature}: {variance:.6f}\")\n",
    "    \n",
    "    # 4. Feature importance by type\n",
    "    print(f\"\\nCategorizing features by type and importance...\")\n",
    "    \n",
    "    # High-value features (complete + correlated)\n",
    "    high_value_features = [\n",
    "        col for col in complete_features \n",
    "        if col in feature_correlations and feature_correlations[col] > 0.02\n",
    "    ]\n",
    "    \n",
    "    # Essential demographic features\n",
    "    essential_demographic = [\n",
    "        'AGE_AT_ADMISSION', 'GENDER', 'RACE_ETHNICITY', 'AGE_CATEGORY'\n",
    "    ]\n",
    "    essential_demographic = [col for col in essential_demographic if col in df.columns]\n",
    "    \n",
    "    # Essential clinical features\n",
    "    essential_clinical = [\n",
    "        'LOS_CALCULATED', 'LOS_CATEGORY', 'CLM_DRG_CD', 'ICD9_DGNS_CD_1', \n",
    "        'HIGH_RISK_DRG', 'HIGH_READMISSION_DX'\n",
    "    ]\n",
    "    essential_clinical = [col for col in essential_clinical if col in df.columns]\n",
    "    \n",
    "    # Essential chronic condition features\n",
    "    essential_chronic = [\n",
    "        'CHRONIC_CONDITION_COUNT', 'HAS_CARDIOVASCULAR', 'HAS_METABOLIC',\n",
    "        'HIGH_COMORBIDITY', 'MULTI_SYSTEM_DISEASE'\n",
    "    ]\n",
    "    essential_chronic = [col for col in essential_chronic if col in df.columns]\n",
    "    \n",
    "    # Essential history features\n",
    "    essential_history = [\n",
    "        'PRIOR_ADMISSIONS_365D', 'PRIOR_ADMISSIONS_90D', 'FREQUENT_FLYER',\n",
    "        'RECENT_ADMISSION_30D', 'DAYS_SINCE_LAST_ADMISSION'\n",
    "    ]\n",
    "    essential_history = [col for col in essential_history if col in df.columns]\n",
    "    \n",
    "    # Risk indicators\n",
    "    risk_indicators = [\n",
    "        'COMPREHENSIVE_RISK_SCORE', 'RISK_CATEGORY', 'ELDERLY_COMPLEX',\n",
    "        'COMPLEX_CARDIAC', 'PREMATURE_DISCHARGE_RISK'\n",
    "    ]\n",
    "    risk_indicators = [col for col in risk_indicators if col in df.columns]\n",
    "    \n",
    "    print(f\"Feature categories:\")\n",
    "    print(f\"├── High-value features: {len(high_value_features)}\")\n",
    "    print(f\"├── Essential demographic: {len(essential_demographic)}\")\n",
    "    print(f\"├── Essential clinical: {len(essential_clinical)}\")\n",
    "    print(f\"├── Essential chronic conditions: {len(essential_chronic)}\")\n",
    "    print(f\"├── Essential history: {len(essential_history)}\")\n",
    "    print(f\"└── Risk indicators: {len(risk_indicators)}\")\n",
    "    \n",
    "    # 5. Create modeling feature set\n",
    "    modeling_features = list(set(\n",
    "        essential_demographic + essential_clinical + essential_chronic + \n",
    "        essential_history + risk_indicators + high_value_features\n",
    "    ))\n",
    "    \n",
    "    # Add target variable\n",
    "    modeling_features.append('READMISSION_30DAY')\n",
    "    \n",
    "    # Ensure all features exist in dataframe\n",
    "    modeling_features = [col for col in modeling_features if col in df.columns]\n",
    "    \n",
    "    print(f\"\\nRecommended modeling features: {len(modeling_features)-1} features + target\")\n",
    "    \n",
    "    return modeling_features, {\n",
    "        'complete_features': complete_features,\n",
    "        'high_value_features': high_value_features,\n",
    "        'feature_correlations': sorted_correlations,\n",
    "        'categorical_variance': sorted_categorical\n",
    "    }\n",
    "\n",
    "# Validate and select features\n",
    "modeling_features, feature_analysis = validate_and_select_features(df_with_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a6925",
   "metadata": {},
   "source": [
    "### Block 10: Create Final Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa335496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING FINAL FEATURE DATASET\n",
      "===================================\n",
      "Selected 55 features + target variable\n",
      "\n",
      "Handling missing values in final dataset...\n",
      "Features with missing values:\n",
      "  ICD9_DGNS_CD_1: 95 (0.1%)\n",
      "    → Filled with mode: 486\n",
      "  AGE_CATEGORY: 226 (0.3%)\n",
      "    → Filled with mode: Young_Senior\n",
      "  DAYS_SINCE_LAST_ADMISSION: 39,005 (58.4%)\n",
      "    → Filled with median: 86.0\n",
      "  AGE_AT_ADMISSION: 226 (0.3%)\n",
      "    → Filled with median: 74.98151950718686\n",
      "  GENDER: 226 (0.3%)\n",
      "    → Filled with mode: Female\n",
      "  RACE_ETHNICITY: 226 (0.3%)\n",
      "    → Filled with mode: White\n",
      "\n",
      "Optimizing data types...\n",
      "  ✓ Data types optimized\n",
      "  Final memory usage: 7.1 MB\n",
      "\n",
      "FINAL DATASET SUMMARY:\n",
      "├── Records: 66,773\n",
      "├── Features: 55\n",
      "├── Target variable: READMISSION_30DAY\n",
      "├── Readmission rate: 10.13%\n",
      "├── Memory usage: 7.1 MB\n",
      "└── Data types: {dtype('uint8'): np.int64(38), dtype('int32'): np.int64(7), dtype('float64'): np.int64(3), CategoricalDtype(categories=['Low_Risk', 'Moderate_Risk', 'High_Risk', 'Very_High_Risk'], ordered=True, categories_dtype=object): np.int64(1), CategoricalDtype(categories=['0030', '0031', '0039', '0049', '0059', '0071', '00842',\n",
      "                  '00843', '00845', '00846',\n",
      "                  ...\n",
      "                  'V5877', 'V5878', 'V5881', 'V5889', 'V6284', 'V652', 'V714',\n",
      "                  'V717', 'V7189', 'V7283'],\n",
      ", ordered=False, categories_dtype=object): np.int64(1), CategoricalDtype(categories=['Under_65', 'Young_Senior', 'Old_Senior', 'Very_Old'], ordered=True, categories_dtype=object): np.int64(1), CategoricalDtype(categories=['000', '001', '002', '003', '004', '005', '006', '007',\n",
      "                  '008', '009',\n",
      "                  ...\n",
      "                  '983', '984', '985', '986', '987', '988', '989', '998',\n",
      "                  '999', 'OTH'],\n",
      ", ordered=False, categories_dtype=object): np.int64(1), CategoricalDtype(categories=['Same_Day', 'Short_2-3', 'Medium_4-7', 'Long_8-14',\n",
      "                  'Very_Long_15-30', 'Extended_30+'],\n",
      ", ordered=True, categories_dtype=object): np.int64(1), CategoricalDtype(categories=['Female', 'Male'], ordered=False, categories_dtype=object): np.int64(1), CategoricalDtype(categories=['Black', 'Hispanic', 'Other', 'White'], ordered=False, categories_dtype=object): np.int64(1), dtype('int64'): np.int64(1)}\n",
      "\n",
      "FEATURE TYPE BREAKDOWN:\n",
      "├── Demographic: 4 features\n",
      "├── Clinical: 10 features\n",
      "├── Chronic_Conditions: 5 features\n",
      "├── Prior_History: 9 features\n",
      "├── Risk_Indicators: 10 features\n",
      "├── Temporal: 18 features\n",
      "├── Other: 12 features\n"
     ]
    }
   ],
   "source": [
    "def create_final_feature_dataset(df, modeling_features):\n",
    "    \"\"\"\n",
    "    Create final dataset optimized for machine learning\n",
    "    \"\"\"\n",
    "    print(\"CREATING FINAL FEATURE DATASET\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Select modeling features\n",
    "    df_final = df[modeling_features].copy()\n",
    "    \n",
    "    print(f\"Selected {len(modeling_features)-1} features + target variable\")\n",
    "    \n",
    "    # Handle any remaining missing values\n",
    "    print(f\"\\nHandling missing values in final dataset...\")\n",
    "    \n",
    "    missing_summary = df_final.isnull().sum()\n",
    "    features_with_missing = missing_summary[missing_summary > 0]\n",
    "    \n",
    "    if len(features_with_missing) > 0:\n",
    "        print(f\"Features with missing values:\")\n",
    "        for feature, count in features_with_missing.items():\n",
    "            pct = count / len(df_final) * 100\n",
    "            print(f\"  {feature}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Handle missing values based on feature type\n",
    "            if df_final[feature].dtype in ['int64', 'float64']:\n",
    "                # Numeric features: fill with median\n",
    "                median_val = df_final[feature].median()\n",
    "                df_final[feature].fillna(median_val, inplace=True)\n",
    "                print(f\"    → Filled with median: {median_val}\")\n",
    "            else:\n",
    "                # Categorical features: fill with mode or 'Unknown'\n",
    "                if df_final[feature].mode().empty:\n",
    "                    df_final[feature].fillna('Unknown', inplace=True)\n",
    "                    print(f\"    → Filled with 'Unknown'\")\n",
    "                else:\n",
    "                    mode_val = df_final[feature].mode()[0]\n",
    "                    df_final[feature].fillna(mode_val, inplace=True)\n",
    "                    print(f\"    → Filled with mode: {mode_val}\")\n",
    "    else:\n",
    "        print(\"✓ No missing values in final dataset\")\n",
    "    \n",
    "    # Data type optimization\n",
    "    print(f\"\\nOptimizing data types...\")\n",
    "    \n",
    "    # Convert boolean columns to int8\n",
    "    bool_columns = df_final.select_dtypes(include=['bool']).columns\n",
    "    if len(bool_columns) > 0:\n",
    "        for col in bool_columns:\n",
    "            df_final[col] = df_final[col].astype('int8')\n",
    "        print(f\"  ✓ Converted {len(bool_columns)} boolean columns to int8\")\n",
    "    \n",
    "    # Optimize integer columns\n",
    "    int_columns = df_final.select_dtypes(include=['int64']).columns\n",
    "    for col in int_columns:\n",
    "        if col != 'READMISSION_30DAY':  # Keep target as int64\n",
    "            max_val = df_final[col].max()\n",
    "            min_val = df_final[col].min()\n",
    "            \n",
    "            if min_val >= 0 and max_val <= 255:\n",
    "                df_final[col] = df_final[col].astype('uint8')\n",
    "            elif min_val >= -128 and max_val <= 127:\n",
    "                df_final[col] = df_final[col].astype('int8')\n",
    "            elif min_val >= -32768 and max_val <= 32767:\n",
    "                df_final[col] = df_final[col].astype('int16')\n",
    "            elif min_val >= -2147483648 and max_val <= 2147483647:\n",
    "                df_final[col] = df_final[col].astype('int32')\n",
    "    \n",
    "    # Convert high-cardinality categorical columns to category dtype\n",
    "    categorical_columns = df_final.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        unique_values = df_final[col].nunique()\n",
    "        if unique_values < len(df_final) * 0.5:  # Less than 50% unique values\n",
    "            df_final[col] = df_final[col].astype('category')\n",
    "    \n",
    "    print(f\"  ✓ Data types optimized\")\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    memory_usage = df_final.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"  Final memory usage: {memory_usage:.1f} MB\")\n",
    "    \n",
    "    # Feature summary\n",
    "    print(f\"\\nFINAL DATASET SUMMARY:\")\n",
    "    print(f\"├── Records: {len(df_final):,}\")\n",
    "    print(f\"├── Features: {len(df_final.columns)-1}\")\n",
    "    print(f\"├── Target variable: READMISSION_30DAY\")\n",
    "    print(f\"├── Readmission rate: {df_final['READMISSION_30DAY'].mean()*100:.2f}%\")\n",
    "    print(f\"├── Memory usage: {memory_usage:.1f} MB\")\n",
    "    print(f\"└── Data types: {dict(df_final.dtypes.value_counts())}\")\n",
    "    \n",
    "    # Feature type breakdown\n",
    "    feature_types = {\n",
    "        'Demographic': [col for col in df_final.columns if any(x in col.lower() for x in ['age', 'gender', 'race', 'sex'])],\n",
    "        'Clinical': [col for col in df_final.columns if any(x in col.lower() for x in ['los', 'drg', 'dx', 'icd9', 'stay'])],\n",
    "        'Chronic_Conditions': [col for col in df_final.columns if col.startswith('SP_') or 'condition' in col.lower() or 'comorbid' in col.lower()],\n",
    "        'Prior_History': [col for col in df_final.columns if 'prior' in col.lower() or 'frequent' in col.lower() or 'days_since' in col.lower()],\n",
    "        'Risk_Indicators': [col for col in df_final.columns if 'risk' in col.lower() or 'complex' in col.lower()],\n",
    "        'Temporal': [col for col in df_final.columns if any(x in col.lower() for x in ['admission', 'discharge', 'weekend', 'season'])],\n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    # Classify remaining features\n",
    "    classified_features = set()\n",
    "    for category, features in feature_types.items():\n",
    "        classified_features.update(features)\n",
    "    \n",
    "    feature_types['Other'] = [col for col in df_final.columns if col not in classified_features and col != 'READMISSION_30DAY']\n",
    "    \n",
    "    print(f\"\\nFEATURE TYPE BREAKDOWN:\")\n",
    "    for category, features in feature_types.items():\n",
    "        if features:\n",
    "            print(f\"├── {category}: {len(features)} features\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Create final feature dataset\n",
    "df_final_features = create_final_feature_dataset(df_with_risk, modeling_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1c5af",
   "metadata": {},
   "source": [
    "### Block 11: Save Feature Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a73e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTEMPTING TO SAVE FEATURE DATASET...\n",
      "SAVING FEATURE-ENGINEERED DATASET\n",
      "========================================\n",
      "✓ Feature dataset saved:\n",
      "  ├── CSV: data/features/readmission_features_final.csv\n",
      "  └── Parquet: data/features/readmission_features_final.parquet\n",
      "✓ Feature list saved: data/features/modeling_features_list.txt\n",
      "✓ Documentation saved: data/features/feature_engineering_documentation.txt\n",
      "Creating correlation matrix for numeric features...\n",
      "✓ Correlation matrix saved: data/features/feature_correlation_matrix.csv\n",
      "  Features included: 49\n",
      "✓ Summary saved: data/features/feature_engineering_summary.txt\n",
      "\n",
      "🎉 All files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_feature_dataset(df_final, feature_analysis):\n",
    "    \"\"\"\n",
    "    Save the final feature-engineered dataset and documentation\n",
    "    \"\"\"\n",
    "    print(\"SAVING FEATURE-ENGINEERED DATASET\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Create features directory\n",
    "    features_path = \"data/features/\"\n",
    "    os.makedirs(features_path, exist_ok=True)\n",
    "    \n",
    "    # Save main feature dataset\n",
    "    feature_file_csv = f\"{features_path}readmission_features_final.csv\"\n",
    "    feature_file_parquet = f\"{features_path}readmission_features_final.parquet\"\n",
    "    \n",
    "    df_final.to_csv(feature_file_csv, index=False)\n",
    "    df_final.to_parquet(feature_file_parquet)\n",
    "    \n",
    "    print(f\"✓ Feature dataset saved:\")\n",
    "    print(f\"  ├── CSV: {feature_file_csv}\")\n",
    "    print(f\"  └── Parquet: {feature_file_parquet}\")\n",
    "    \n",
    "    # Save feature list for modeling\n",
    "    feature_list = [col for col in df_final.columns if col != 'READMISSION_30DAY']\n",
    "    feature_list_file = f\"{features_path}modeling_features_list.txt\"\n",
    "    \n",
    "    with open(feature_list_file, 'w') as f:\n",
    "        f.write(\"MODELING FEATURES LIST\\n\")\n",
    "        f.write(\"=\"*30 + \"\\n\\n\")\n",
    "        f.write(f\"Total Features: {len(feature_list)}\\n\")\n",
    "        f.write(f\"Target Variable: READMISSION_30DAY\\n\")\n",
    "        f.write(f\"Readmission Rate: {df_final['READMISSION_30DAY'].mean()*100:.2f}%\\n\\n\")\n",
    "        f.write(\"Features by Category:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        \n",
    "        # Categorize features that actually exist\n",
    "        feature_categories = {\n",
    "            'Demographic': [col for col in feature_list if any(x in col.lower() for x in ['age', 'gender', 'race', 'sex', 'male', 'female', 'white', 'black', 'minority'])],\n",
    "            'Clinical': [col for col in feature_list if any(x in col.lower() for x in ['los', 'drg', 'dx', 'icd9', 'stay', 'admission', 'discharge'])],\n",
    "            'Chronic_Conditions': [col for col in feature_list if col.startswith('SP_') or any(x in col.lower() for x in ['condition', 'comorbid', 'chronic', 'cardiovascular', 'metabolic', 'diabetes', 'chf'])],\n",
    "            'Prior_History': [col for col in feature_list if any(x in col.lower() for x in ['prior', 'frequent', 'days_since', 'recent', 'utiliz'])],\n",
    "            'Risk_Indicators': [col for col in feature_list if any(x in col.lower() for x in ['risk', 'complex', 'score', 'high_risk'])],\n",
    "            'Temporal': [col for col in feature_list if any(x in col.lower() for x in ['weekend', 'season', 'month', 'quarter', 'year', 'day_of_week', 'holiday'])],\n",
    "        }\n",
    "        \n",
    "        # Add uncategorized features\n",
    "        categorized = set()\n",
    "        for category, features in feature_categories.items():\n",
    "            categorized.update(features)\n",
    "        \n",
    "        feature_categories['Other'] = [col for col in feature_list if col not in categorized]\n",
    "        \n",
    "        # Write categorized features\n",
    "        for category, features in feature_categories.items():\n",
    "            if features:\n",
    "                f.write(f\"\\n{category} ({len(features)} features):\\n\")\n",
    "                for i, feature in enumerate(features, 1):\n",
    "                    f.write(f\"  {i:2d}. {feature}\\n\")\n",
    "    \n",
    "    print(f\"✓ Feature list saved: {feature_list_file}\")\n",
    "    \n",
    "    # Save feature engineering documentation\n",
    "    documentation_file = f\"{features_path}feature_engineering_documentation.txt\"\n",
    "    \n",
    "    with open(documentation_file, 'w') as f:\n",
    "        f.write(\"FEATURE ENGINEERING DOCUMENTATION\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"DATASET OVERVIEW:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        f.write(f\"Total Records: {len(df_final):,}\\n\")\n",
    "        f.write(f\"Total Features: {len(df_final.columns)-1}\\n\")\n",
    "        f.write(f\"Target Variable: READMISSION_30DAY\\n\")\n",
    "        f.write(f\"Readmission Rate: {df_final['READMISSION_30DAY'].mean()*100:.2f}%\\n\")\n",
    "        f.write(f\"Memory Usage: {df_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\\n\")\n",
    "        f.write(f\"Missing Values: {df_final.isnull().sum().sum()}\\n\\n\")\n",
    "        \n",
    "        f.write(\"FEATURE CREATION PROCESS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(\"1. Enhanced Demographic Features\\n\")\n",
    "        f.write(\"   - Age groups and risk categories\\n\")\n",
    "        f.write(\"   - Gender encoding\\n\")\n",
    "        f.write(\"   - Race/ethnicity grouping\\n\")\n",
    "        f.write(\"   - Geographic features\\n\")\n",
    "        f.write(\"   - ESRD indicators\\n\\n\")\n",
    "        \n",
    "        f.write(\"2. Advanced Clinical Features\\n\")\n",
    "        f.write(\"   - Length of stay categories\\n\")\n",
    "        f.write(\"   - DRG frequency and risk indicators\\n\")\n",
    "        f.write(\"   - Primary diagnosis features\\n\")\n",
    "        f.write(\"   - Admission timing features\\n\\n\")\n",
    "        \n",
    "        f.write(\"3. Chronic Condition Features\\n\")\n",
    "        f.write(\"   - Individual condition analysis\\n\")\n",
    "        f.write(\"   - Condition severity categories\\n\")\n",
    "        f.write(\"   - Comorbidity burden indicators\\n\")\n",
    "        f.write(\"   - High-risk condition combinations\\n\\n\")\n",
    "        \n",
    "        f.write(\"4. Prior Admission History\\n\")\n",
    "        f.write(\"   - Prior admissions in multiple time windows\\n\")\n",
    "        f.write(\"   - Days since last admission\\n\")\n",
    "        f.write(\"   - Frequent flyer identification\\n\")\n",
    "        f.write(\"   - Prior length of stay metrics\\n\\n\")\n",
    "        \n",
    "        f.write(\"5. Advanced Risk Indicators\\n\")\n",
    "        f.write(\"   - Composite risk scores\\n\")\n",
    "        f.write(\"   - High-risk patient profiles\\n\")\n",
    "        f.write(\"   - Discharge timing risk factors\\n\")\n",
    "        f.write(\"   - Social determinants proxies\\n\\n\")\n",
    "        \n",
    "        # Only include validation results if they exist\n",
    "        if 'feature_correlations' in feature_analysis and feature_analysis['feature_correlations']:\n",
    "            f.write(\"TOP CORRELATED FEATURES:\\n\")\n",
    "            for i, (feature, corr) in enumerate(feature_analysis['feature_correlations'][:10], 1):\n",
    "                if feature in df_final.columns:  # Only include features that exist\n",
    "                    f.write(f\"{i:2d}. {feature}: {corr:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"DATA TYPES DISTRIBUTION:\\n\")\n",
    "        dtype_counts = df_final.dtypes.value_counts()\n",
    "        for dtype, count in dtype_counts.items():\n",
    "            f.write(f\"- {dtype}: {count} features\\n\")\n",
    "    \n",
    "    print(f\"✓ Documentation saved: {documentation_file}\")\n",
    "    \n",
    "    # Create correlation matrix only for numeric features that exist\n",
    "    print(\"Creating correlation matrix for numeric features...\")\n",
    "    numeric_features = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numeric_features) > 1:\n",
    "        try:\n",
    "            correlation_matrix = df_final[numeric_features].corr()\n",
    "            correlation_file = f\"{features_path}feature_correlation_matrix.csv\"\n",
    "            correlation_matrix.to_csv(correlation_file)\n",
    "            print(f\"✓ Correlation matrix saved: {correlation_file}\")\n",
    "            print(f\"  Features included: {len(numeric_features)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not create correlation matrix: {e}\")\n",
    "    else:\n",
    "        print(\"⚠️ Not enough numeric features for correlation matrix\")\n",
    "    \n",
    "    # Create summary file\n",
    "    summary_file = f\"{features_path}feature_engineering_summary.txt\"\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"FEATURE ENGINEERING SUMMARY\\n\")\n",
    "        f.write(\"=\"*40 + \"\\n\\n\")\n",
    "        f.write(f\"FINAL DATASET STATISTICS:\\n\")\n",
    "        f.write(f\"- Records: {len(df_final):,}\\n\")\n",
    "        f.write(f\"- Features: {len(df_final.columns)-1}\\n\")\n",
    "        f.write(f\"- Target: READMISSION_30DAY\\n\")\n",
    "        f.write(f\"- Readmission Rate: {df_final['READMISSION_30DAY'].mean()*100:.2f}%\\n\")\n",
    "        f.write(f\"- Memory Usage: {df_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\\n\")\n",
    "        f.write(f\"- Missing Values: {df_final.isnull().sum().sum()}\\n\\n\")\n",
    "        f.write(f\"STATUS: Ready for Model Development (Phase E)\\n\")\n",
    "    \n",
    "    print(f\"✓ Summary saved: {summary_file}\")\n",
    "\n",
    "# Save feature dataset with proper error handling\n",
    "print(\"ATTEMPTING TO SAVE FEATURE DATASET...\")\n",
    "try:\n",
    "    save_feature_dataset(df_final_features, feature_analysis)\n",
    "    print(\"\\n All files saved successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error during save: {e}\")\n",
    "    print(\"Performing basic save...\")\n",
    "    \n",
    "    # Backup basic save\n",
    "    features_path = \"data/features/\"\n",
    "    os.makedirs(features_path, exist_ok=True)\n",
    "    \n",
    "    df_final_features.to_csv(f\"{features_path}readmission_features_final.csv\", index=False)\n",
    "    df_final_features.to_parquet(f\"{features_path}readmission_features_final.parquet\")\n",
    "    \n",
    "    print(\"✓ Basic dataset files saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af2f0a",
   "metadata": {},
   "source": [
    "### Block 12: Feature Engineering Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e54e2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL FEATURE ENGINEERING VALIDATION\n",
      "=============================================\n",
      "Data Quality Validation:\n",
      "├── Missing values: 0 (should be 0)\n",
      "├── Infinite values: 0 ✓\n",
      "├── Target distribution: {0: np.int64(60006), 1: np.int64(6767)}\n",
      "└── Class balance: 10.1% positive class\n",
      "\n",
      "Feature Range Validation:\n",
      "├── AGE_AT_ADMISSION: [24.6, 101.7] ✓\n",
      "├── LOS_CALCULATED: [1.0, 36.0] ✓\n",
      "├── CHRONIC_CONDITION_COUNT: [0.0, 11.0] ✓\n",
      "├── PRIOR_ADMISSIONS_365D: [0.0, 10.0] ✓\n",
      "├── COMPREHENSIVE_RISK_SCORE: [0.0, 12.0] ✓\n",
      "\n",
      "Feature Correlation Analysis:\n",
      "├── High correlation pairs (>0.8): 27\n",
      "    HIGH_COMORBIDITY ↔ COMORBIDITY_RISK_SCORE: 0.876\n",
      "    HIGH_COMORBIDITY ↔ CHRONIC_CONDITION_COUNT: 0.827\n",
      "    HIGH_COMORBIDITY ↔ COMPLEX_CARDIAC: 0.997\n",
      "    CLM_ADMSN_DT ↔ ADMISSION_YEAR: 0.999\n",
      "    CLM_ADMSN_DT ↔ NCH_BENE_DSCHRG_DT: 0.990\n",
      "\n",
      "Business Logic Validation:\n",
      "├── Comorbidity Risk: 14.6% vs 5.4% ✓\n",
      "├── Frequent Flyer: 15.5% vs 9.8% ✓\n",
      "├── Recent Admission: 14.4% vs 9.7% ✓\n",
      "\n",
      "Model Readiness Assessment:\n",
      "├── Data types: All suitable for modeling ✓\n",
      "├── Feature count: 55 (optimal range) ✓\n",
      "├── Sample size: 66,773 (adequate) ✓\n",
      "└── Memory efficiency: 7.1 MB\n",
      "\n",
      "FINAL RECOMMENDATIONS:\n",
      "├── Data quality: Excellent ✓\n",
      "├── Multicollinearity: Consider feature selection ⚠️\n",
      "├── Class balance: Realistic for healthcare ✓\n",
      "└── Dataset ready for model development ✓\n"
     ]
    }
   ],
   "source": [
    "def final_feature_validation(df_final):\n",
    "    \"\"\"\n",
    "    Perform final validation of the feature-engineered dataset\n",
    "    \"\"\"\n",
    "    print(\"FINAL FEATURE ENGINEERING VALIDATION\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # 1. Data quality checks\n",
    "    print(\"Data Quality Validation:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df_final.isnull().sum().sum()\n",
    "    print(f\"├── Missing values: {missing_values} (should be 0)\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "    infinite_values = 0\n",
    "    for col in numeric_cols:\n",
    "        inf_count = np.isinf(df_final[col]).sum()\n",
    "        infinite_values += inf_count\n",
    "        if inf_count > 0:\n",
    "            print(f\"├── Infinite values in {col}: {inf_count}\")\n",
    "    \n",
    "    if infinite_values == 0:\n",
    "        print(f\"├── Infinite values: 0 ✓\")\n",
    "    else:\n",
    "        print(f\"├── Infinite values: {infinite_values} ⚠️\")\n",
    "    \n",
    "    # Check target variable distribution\n",
    "    target_dist = df_final['READMISSION_30DAY'].value_counts().sort_index()\n",
    "    print(f\"├── Target distribution: {dict(target_dist)}\")\n",
    "    print(f\"└── Class balance: {target_dist[1]/len(df_final)*100:.1f}% positive class\")\n",
    "    \n",
    "    # 2. Feature range validation\n",
    "    print(f\"\\nFeature Range Validation:\")\n",
    "    \n",
    "    # Check for reasonable ranges in key features\n",
    "    key_validations = {\n",
    "        'AGE_AT_ADMISSION': (0, 120),\n",
    "        'LOS_CALCULATED': (0, 365),\n",
    "        'CHRONIC_CONDITION_COUNT': (0, 20),\n",
    "        'PRIOR_ADMISSIONS_365D': (0, 50),\n",
    "        'COMPREHENSIVE_RISK_SCORE': (0, 20)\n",
    "    }\n",
    "    \n",
    "    for feature, (min_expected, max_expected) in key_validations.items():\n",
    "        if feature in df_final.columns:\n",
    "            actual_min = df_final[feature].min()\n",
    "            actual_max = df_final[feature].max()\n",
    "            \n",
    "            if min_expected <= actual_min and actual_max <= max_expected:\n",
    "                status = \"✓\"\n",
    "            else:\n",
    "                status = \"⚠️\"\n",
    "            \n",
    "            print(f\"├── {feature}: [{actual_min:.1f}, {actual_max:.1f}] {status}\")\n",
    "    \n",
    "    # 3. Feature correlation analysis\n",
    "    print(f\"\\nFeature Correlation Analysis:\")\n",
    "    \n",
    "    # Check for highly correlated features (potential multicollinearity)\n",
    "    correlation_matrix = df_final[numeric_cols].corr()\n",
    "    \n",
    "    # Find pairs with high correlation (>0.8, excluding self-correlation)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = abs(correlation_matrix.iloc[i, j])\n",
    "            if corr_val > 0.8:\n",
    "                feature1 = correlation_matrix.columns[i]\n",
    "                feature2 = correlation_matrix.columns[j]\n",
    "                high_corr_pairs.append((feature1, feature2, corr_val))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"├── High correlation pairs (>0.8): {len(high_corr_pairs)}\")\n",
    "        for feat1, feat2, corr in high_corr_pairs[:5]:  # Show first 5\n",
    "            print(f\"    {feat1} ↔ {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"├── High correlation pairs (>0.8): 0 ✓\")\n",
    "    \n",
    "    # 4. Business logic validation\n",
    "    print(f\"\\nBusiness Logic Validation:\")\n",
    "    \n",
    "    # Readmission rates should increase with risk indicators\n",
    "    risk_validations = [\n",
    "        ('HIGH_RISK_AGE', 'Age Risk'),\n",
    "        ('HIGH_COMORBIDITY', 'Comorbidity Risk'),\n",
    "        ('FREQUENT_FLYER', 'Frequent Flyer'),\n",
    "        ('RECENT_ADMISSION_30D', 'Recent Admission')\n",
    "    ]\n",
    "    \n",
    "    overall_rate = df_final['READMISSION_30DAY'].mean()\n",
    "    \n",
    "    for risk_feature, description in risk_validations:\n",
    "        if risk_feature in df_final.columns:\n",
    "            high_risk_rate = df_final[df_final[risk_feature] == 1]['READMISSION_30DAY'].mean()\n",
    "            low_risk_rate = df_final[df_final[risk_feature] == 0]['READMISSION_30DAY'].mean()\n",
    "            \n",
    "            if high_risk_rate > low_risk_rate:\n",
    "                status = \"✓\"\n",
    "            else:\n",
    "                status = \"⚠️\"\n",
    "            \n",
    "            print(f\"├── {description}: {high_risk_rate*100:.1f}% vs {low_risk_rate*100:.1f}% {status}\")\n",
    "    \n",
    "    # 5. Model readiness assessment\n",
    "    print(f\"\\nModel Readiness Assessment:\")\n",
    "    \n",
    "    # Check data types\n",
    "    suitable_types = ['int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64', \n",
    "                     'float32', 'float64', 'category']\n",
    "    \n",
    "    unsuitable_features = []\n",
    "    for col in df_final.columns:\n",
    "        if col != 'READMISSION_30DAY' and str(df_final[col].dtype) not in suitable_types:\n",
    "            unsuitable_features.append((col, df_final[col].dtype))\n",
    "    \n",
    "    if unsuitable_features:\n",
    "        print(f\"├── Unsuitable data types: {len(unsuitable_features)}\")\n",
    "        for feat, dtype in unsuitable_features:\n",
    "            print(f\"    {feat}: {dtype}\")\n",
    "    else:\n",
    "        print(f\"├── Data types: All suitable for modeling ✓\")\n",
    "    \n",
    "    # Check feature count\n",
    "    feature_count = len(df_final.columns) - 1  # Exclude target\n",
    "    if 20 <= feature_count <= 100:\n",
    "        print(f\"├── Feature count: {feature_count} (optimal range) ✓\")\n",
    "    elif feature_count < 20:\n",
    "        print(f\"├── Feature count: {feature_count} (may need more features) ⚠️\")\n",
    "    else:\n",
    "        print(f\"├── Feature count: {feature_count} (consider feature selection) ⚠️\")\n",
    "    \n",
    "    # Check sample size\n",
    "    sample_size = len(df_final)\n",
    "    min_samples = feature_count * 20  # Rule of thumb: 20 samples per feature\n",
    "    \n",
    "    if sample_size >= min_samples:\n",
    "        print(f\"├── Sample size: {sample_size:,} (adequate) ✓\")\n",
    "    else:\n",
    "        print(f\"├── Sample size: {sample_size:,} (may be insufficient) ⚠️\")\n",
    "    \n",
    "    print(f\"└── Memory efficiency: {df_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # 6. Final recommendations\n",
    "    print(f\"\\nFINAL RECOMMENDATIONS:\")\n",
    "    \n",
    "    if missing_values == 0 and infinite_values == 0:\n",
    "        print(\"├── Data quality: Excellent ✓\")\n",
    "    else:\n",
    "        print(\"├── Data quality: Issues detected - review before modeling ⚠️\")\n",
    "    \n",
    "    if len(high_corr_pairs) <= 3:\n",
    "        print(\"├── Multicollinearity: Acceptable ✓\")\n",
    "    else:\n",
    "        print(\"├── Multicollinearity: Consider feature selection ⚠️\")\n",
    "    \n",
    "    if 10 <= df_final['READMISSION_30DAY'].mean()*100 <= 20:\n",
    "        print(\"├── Class balance: Realistic for healthcare ✓\")\n",
    "    else:\n",
    "        print(\"├── Class balance: May need resampling techniques ⚠️\")\n",
    "    \n",
    "    print(\"└── Dataset ready for model development ✓\")\n",
    "    \n",
    "    return {\n",
    "        'data_quality_score': 100 - (missing_values + infinite_values),\n",
    "        'feature_count': feature_count,\n",
    "        'sample_size': sample_size,\n",
    "        'readmission_rate': df_final['READMISSION_30DAY'].mean(),\n",
    "        'memory_usage_mb': df_final.memory_usage(deep=True).sum() / 1024**2\n",
    "    }\n",
    "\n",
    "# Perform final validation\n",
    "validation_results = final_feature_validation(df_final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98a7b2",
   "metadata": {},
   "source": [
    "### Block 13: Phase D Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75f8bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE D COMPLETE: FEATURE ENGINEERING\n",
      "============================================================\n",
      "✅ ACCOMPLISHMENTS:\n",
      "├── Enhanced demographic features (age groups, race, geography)\n",
      "├── Advanced clinical features (LOS categories, DRG risk, diagnosis)\n",
      "├── Comprehensive chronic condition features\n",
      "├── Prior admission history analysis\n",
      "├── Advanced risk indicators and composite scores\n",
      "├── Feature validation and selection\n",
      "├── Data type optimization and memory efficiency\n",
      "└── Model-ready dataset creation\n",
      "\n",
      "📊 FINAL FEATURE DATASET STATISTICS:\n",
      "├── Total records: 66,773\n",
      "├── Engineered features: 55\n",
      "├── Target variable: READMISSION_30DAY\n",
      "├── Readmission rate: 10.13%\n",
      "├── Memory usage: 7.1 MB\n",
      "├── Data quality score: 100/100\n",
      "└── Missing values: 0\n",
      "\n",
      "🎯 FEATURE CATEGORIES CREATED:\n",
      "├── Demographic: 4 features\n",
      "├── Clinical: 8 features\n",
      "├── Chronic Conditions: 1 features\n",
      "├── Prior History: 8 features\n",
      "├── Risk Indicators: 10 features\n",
      "├── Temporal: 15 features\n",
      "\n",
      "🔬 MODEL READINESS ASSESSMENT:\n",
      "├── Data quality: Excellent ✓\n",
      "├── Feature count: Optimal range ✓\n",
      "├── Sample size: Adequate for modeling ✓\n",
      "├── Readmission rate: Realistic range ✓\n",
      "└── Overall assessment: Ready for modeling ✓\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "├── Ready for Notebook 5: Model Development\n",
      "├── Train multiple machine learning algorithms\n",
      "├── Hyperparameter tuning and optimization\n",
      "├── Model evaluation and comparison\n",
      "├── Feature importance analysis\n",
      "├── Business impact calculation\n",
      "└── Model interpretation and validation\n",
      "\n",
      "✓ Phase D Complete - Feature Engineering Successful!\n",
      "✓ Dataset saved: data/features/readmission_features_final.parquet\n",
      "✓ 55 high-quality features ready for machine learning!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE D COMPLETE: FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"✅ ACCOMPLISHMENTS:\")\n",
    "print(\"├── Enhanced demographic features (age groups, race, geography)\")\n",
    "print(\"├── Advanced clinical features (LOS categories, DRG risk, diagnosis)\")\n",
    "print(\"├── Comprehensive chronic condition features\")\n",
    "print(\"├── Prior admission history analysis\")\n",
    "print(\"├── Advanced risk indicators and composite scores\")\n",
    "print(\"├── Feature validation and selection\")\n",
    "print(\"├── Data type optimization and memory efficiency\")\n",
    "print(\"└── Model-ready dataset creation\")\n",
    "\n",
    "print(f\"\\n📊 FINAL FEATURE DATASET STATISTICS:\")\n",
    "feature_count = len(df_final_features.columns) - 1\n",
    "readmission_rate = df_final_features['READMISSION_30DAY'].mean() * 100\n",
    "memory_usage = df_final_features.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"├── Total records: {len(df_final_features):,}\")\n",
    "print(f\"├── Engineered features: {feature_count}\")\n",
    "print(f\"├── Target variable: READMISSION_30DAY\")\n",
    "print(f\"├── Readmission rate: {readmission_rate:.2f}%\")\n",
    "print(f\"├── Memory usage: {memory_usage:.1f} MB\")\n",
    "print(f\"├── Data quality score: {validation_results['data_quality_score']:.0f}/100\")\n",
    "print(f\"└── Missing values: {df_final_features.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\n🎯 FEATURE CATEGORIES CREATED:\")\n",
    "feature_categories = {\n",
    "    'Demographic': len([col for col in df_final_features.columns if any(x in col.lower() for x in ['age', 'gender', 'race', 'sex'])]),\n",
    "    'Clinical': len([col for col in df_final_features.columns if any(x in col.lower() for x in ['los', 'drg', 'dx', 'icd9'])]),\n",
    "    'Chronic Conditions': len([col for col in df_final_features.columns if col.startswith('SP_') or 'condition' in col.lower()]),\n",
    "    'Prior History': len([col for col in df_final_features.columns if 'prior' in col.lower() or 'frequent' in col.lower()]),\n",
    "    'Risk Indicators': len([col for col in df_final_features.columns if 'risk' in col.lower() or 'complex' in col.lower()]),\n",
    "    'Temporal': len([col for col in df_final_features.columns if any(x in col.lower() for x in ['admission', 'weekend', 'season'])])\n",
    "}\n",
    "\n",
    "for category, count in feature_categories.items():\n",
    "    if count > 0:\n",
    "        print(f\"├── {category}: {count} features\")\n",
    "\n",
    "print(f\"\\n🔬 MODEL READINESS ASSESSMENT:\")\n",
    "if validation_results['data_quality_score'] >= 95:\n",
    "    print(\"├── Data quality: Excellent ✓\")\n",
    "elif validation_results['data_quality_score'] >= 85:\n",
    "    print(\"├── Data quality: Good ✓\")\n",
    "else:\n",
    "    print(\"├── Data quality: Needs attention ⚠️\")\n",
    "\n",
    "if 20 <= feature_count <= 100:\n",
    "    print(\"├── Feature count: Optimal range ✓\")\n",
    "else:\n",
    "    print(\"├── Feature count: Consider adjustment ⚠️\")\n",
    "\n",
    "if validation_results['sample_size'] >= feature_count * 10:\n",
    "    print(\"├── Sample size: Adequate for modeling ✓\")\n",
    "else:\n",
    "    print(\"├── Sample size: May be insufficient ⚠️\")\n",
    "\n",
    "if 8 <= readmission_rate <= 20:\n",
    "    print(\"├── Readmission rate: Realistic range ✓\")\n",
    "else:\n",
    "    print(\"├── Readmission rate: Outside expected range ⚠️\")\n",
    "\n",
    "print(\"└── Overall assessment: Ready for modeling ✓\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"├── Ready for Notebook 5: Model Development\")\n",
    "print(\"├── Train multiple machine learning algorithms\")\n",
    "print(\"├── Hyperparameter tuning and optimization\")\n",
    "print(\"├── Model evaluation and comparison\")\n",
    "print(\"├── Feature importance analysis\")\n",
    "print(\"├── Business impact calculation\")\n",
    "print(\"└── Model interpretation and validation\")\n",
    "\n",
    "print(f\"\\n✓ Phase D Complete - Feature Engineering Successful!\")\n",
    "print(f\"✓ Dataset saved: data/features/readmission_features_final.parquet\")\n",
    "print(f\"✓ {feature_count} high-quality features ready for machine learning!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
